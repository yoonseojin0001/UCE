{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lpips\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline):\n",
    "    del pipeline.vae\n",
    "    del pipeline.tokenizer\n",
    "    del pipeline.text_encoder\n",
    "    del pipeline.unet\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def print_cuda():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    for gpu_idx in range(gpu_count):\n",
    "        print(f\"GPU {gpu_idx}: {torch.cuda.get_device_name(gpu_idx)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(gpu_idx) / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(gpu_idx) / 1024 / 1024:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generateImages(pipeline, prompts, prompt_path, save_path,\n",
    "                   sample_count=5, guidance_scale=7.5,):\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    config = {\n",
    "        \"sample_count\": sample_count,\n",
    "        \"prompt_count\": len(prompts),\n",
    "        \"guidance_scale\": guidance_scale\n",
    "    }\n",
    "    with open(f\"{save_path}/config.yaml\", 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "    device = pipeline.unet.device\n",
    "\n",
    "    if prompt_path:\n",
    "        df = pd.read_csv(prompt_path)\n",
    "        prompts = df.prompt\n",
    "        seeds = df.seed\n",
    "    else:\n",
    "        seeds = [random.randint(0, 5000) for _ in range(len(prompts))]\n",
    "\n",
    "    for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "        scheduler.set_timesteps(100)\n",
    "\n",
    "        prompts_ = [prompt] * sample_count + [\"\"] * sample_count\n",
    "\n",
    "        tokens = pipeline.tokenizer(prompts_, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        embds = pipeline.text_encoder(tokens)[0]\n",
    "\n",
    "        generator = torch.Generator(device=device).manual_seed(int(seeds[idx]))\n",
    "        latents = torch.randn((sample_count, 4, 64, 64), generator=generator).to(device)\n",
    "        latents *= scheduler.init_noise_sigma\n",
    "\n",
    "        for t in scheduler.timesteps:\n",
    "\n",
    "            latent_inputs = torch.cat([latents] * 2)\n",
    "            latent_inputs = scheduler.scale_model_input(latent_inputs, timestep=t)\n",
    "\n",
    "            noises = pipeline.unet(latent_inputs, t, encoder_hidden_states=embds).sample\n",
    "            cond_noises, uncond_noises = noises.chunk(2)\n",
    "            noises = uncond_noises + guidance_scale * (cond_noises - uncond_noises)\n",
    "\n",
    "            latents = scheduler.step(noises, t, latents).prev_sample\n",
    "\n",
    "        latents /= 0.18215\n",
    "        images = pipeline.vae.decode(latents).sample\n",
    "        images = ((images + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        images = images.detach().cpu().numpy()\n",
    "        images = (images * 255).round().astype(\"uint8\")\n",
    "        for idx_, image in enumerate(images):\n",
    "            image = Image.fromarray(image)\n",
    "            image.save(f\"{save_path}/{idx}_{idx_}.png\")\n",
    "\n",
    "    df = pd.DataFrame({\"prompt\": prompts, \"seed\": seeds})\n",
    "    df.to_csv(f\"{save_path}/info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(int(pd.read_csv(\"data/default_1/info.csv\").seed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af46c2548c1b4de38f5a1f86a1e5e2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c02d1473a6244298531e57d5f58b75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != struct c10::Half",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(artists)\n\u001b[0;32m     11\u001b[0m prompts \u001b[38;5;241m=\u001b[39m artists[:\u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m \u001b[43mgenerateImages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m delete_pipeline(pipeline)\n\u001b[0;32m     15\u001b[0m print_cuda()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 46\u001b[0m, in \u001b[0;36mgenerateImages\u001b[1;34m(pipeline, prompts, prompt_path, save_path, sample_count, guidance_scale)\u001b[0m\n\u001b[0;32m     43\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     44\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_inputs, timestep\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m---> 46\u001b[0m noises \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m     47\u001b[0m cond_noises, uncond_noises \u001b[38;5;241m=\u001b[39m noises\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     48\u001b[0m noises \u001b[38;5;241m=\u001b[39m uncond_noises \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (cond_noises \u001b[38;5;241m-\u001b[39m uncond_noises)\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1216\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1214\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1216\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1226\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:1288\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[1;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m-> 1288\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:442\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m    430\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    431\u001b[0m             create_custom_forward(block),\n\u001b[0;32m    432\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[0;32m    440\u001b[0m         )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\attention.py:504\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_single\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    502\u001b[0m         norm_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(norm_hidden_states)\n\u001b[1;32m--> 504\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# 4. Feed-forward\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# i2vgen doesn't have this norm 🤷‍♂️\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:490\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n\u001b[0;32m    488\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:2340\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attn\u001b[38;5;241m.\u001b[39mnorm_cross:\n\u001b[0;32m   2338\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mnorm_encoder_hidden_states(encoder_hidden_states)\n\u001b[1;32m-> 2340\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2341\u001b[0m value \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mto_v(encoder_hidden_states)\n\u001b[0;32m   2343\u001b[0m inner_dim \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != struct c10::Half"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline.unet.to(device).half()\n",
    "pipeline.vae.to(device).half()\n",
    "\n",
    "save_path = \"data/default_1\"\n",
    "\n",
    "df = pd.read_csv(\"data/artists1734_prompts.csv\")\n",
    "artists = list(df.artist.unique())\n",
    "random.shuffle(artists)\n",
    "prompts = artists[:20]\n",
    "generateImages(pipeline, prompts, None, save_path)\n",
    "\n",
    "delete_pipeline(pipeline)\n",
    "print_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935ec37e625540b9a6adf6b22a057616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83eb14971d9e45e6800372c3b7e8b342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m withKey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m erased_pipeline \u001b[38;5;241m=\u001b[39m eraseModel(pipeline, prompts[:\u001b[38;5;241m10\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mart\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m, prompts[\u001b[38;5;241m10\u001b[39m:], lamb, eraseScale, preserveScale, withKey, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/artist10erase_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mgenerateImages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/default_1/info.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/artist10erase_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m delete_pipeline(pipeline)\n\u001b[0;32m     15\u001b[0m print_cuda()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 46\u001b[0m, in \u001b[0;36mgenerateImages\u001b[1;34m(pipeline, prompts, prompt_path, save_path, sample_count, guidance_scale)\u001b[0m\n\u001b[0;32m     43\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     44\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_inputs, timestep\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m---> 46\u001b[0m noises \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m     47\u001b[0m cond_noises, uncond_noises \u001b[38;5;241m=\u001b[39m noises\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     48\u001b[0m noises \u001b[38;5;241m=\u001b[39m uncond_noises \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (cond_noises \u001b[38;5;241m-\u001b[39m uncond_noises)\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1141\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sample \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# 1. time\u001b[39;00m\n\u001b[1;32m-> 1141\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_time_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1142\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding(t_emb, timestep_cond)\n\u001b[0;32m   1143\u001b[0m aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:923\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_time_embed\u001b[1;34m(self, sample, timestep)\u001b[0m\n\u001b[0;32m    921\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([timesteps], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(timesteps\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 923\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtimesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[0;32m    926\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mexpand(sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline.unet.to(device)\n",
    "pipeline.vae.to(device)\n",
    "\n",
    "lamb = 0.5\n",
    "eraseScale = 1\n",
    "preserveScale = 0.1\n",
    "withKey = False\n",
    "\n",
    "erased_pipeline = eraseModel(pipeline, prompts[:10], [\"art\"]*10, prompts[10:], lamb, eraseScale, preserveScale, withKey, \"data/artist10erase_1\")\n",
    "generateImages(pipeline, prompts, \"data/default_1/info.csv\", \"data/artist10erase_1\")\n",
    "\n",
    "delete_pipeline(pipeline)\n",
    "print_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eraseModel(model, prevPrompts, newPrompts, retainPrompts=None, \n",
    "               lamb=0.1, eraseScale=0.1, preserveScale=0.1, withKey=True,\n",
    "               name=\"erase\"):\n",
    "\n",
    "    \"\"\"\n",
    "    prevPrompts에 해당하는 개념을 newPrompts로 편집한 모델 반환.\n",
    "\n",
    "    Parameters:\n",
    "        model (StableDiffusionPipeline): 편집할 모델.\n",
    "        prevPrompts (List[str]): 원래 개념이 저장된 string 리스트.\n",
    "        newPrompts (List[str]): 대체할 개념이 저장된 string 리스트.\n",
    "        retainPrompts (List[str] | None): 보존할 개념이 저장된 string 리스트. Default: None\n",
    "        lamb (float): 정규화 강도. Default: 0.1\n",
    "        eraseScale (float): 개념 편집 강도. Default: 0.1\n",
    "        preserveScale (float): 개념 보존 강도. Default: 0.1\n",
    "        withKey (bool): key 가중치 업데이트 여부. Default: True\n",
    "\n",
    "    Returns:\n",
    "        model (StableDiffusionPipeline)\n",
    "    \"\"\"\n",
    "\n",
    "    device = model.unet.device\n",
    "\n",
    "    # 주어진 모델 unet의 cross-attention layer를 caLayers에 저장.\n",
    "    caLayers = []\n",
    "    for name, module in model.unet.named_modules():\n",
    "        # attn2로 끝나는 모듈이 cross-attention layer.\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        caLayers.append(module)\n",
    "\n",
    "    # value projection layer를 targetLayers에 저장. 논문의 W^old에 해당하는 부분.\n",
    "    # (value projection layer): Linear(in_features=1024, out_features=320, bias=False)\n",
    "    # \"stabilityai/stable-diffusion-2-1-base\" 모델이 기준. \"CompVis/stable-diffusion-v1-4\" 모델은 1024 대신 768을 사용.\n",
    "    valueLayers = [layer.to_v for layer in caLayers]\n",
    "    targetLayers = valueLayers\n",
    "\n",
    "    # withKey=True라면 key projection layer도 추가.\n",
    "    if withKey: \n",
    "        # (key projection layer): Linear(in_features=1024, out_features=320, bias=False)\n",
    "        keyLayers = [layer.to_k for layer in caLayers]\n",
    "        targetLayers += keyLayers\n",
    "    \n",
    "    # 텍스트 prevPrompts를 텍스트 임베딩 prevEmbds으로 변환. prevEmbds의 원소는 논문의 c_i에 대응됨.\n",
    "    # (prevPrompts): (N,)\n",
    "    # ex) N = 2; prevPrompts = ['Kelly Mckernan', 'Sarah Anderson']\n",
    "    # (prevEmbds): (N, 1024, 77)\n",
    "    prevInputs = model.tokenizer(prevPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    prevEmbds = model.text_encoder(prevInputs)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "\n",
    "    # 마찬가지로 newEmbds 생성. newEmbds의 원소는 논문의 c*_i에 대응됨.\n",
    "    # (newPrompts): (N,)\n",
    "    # ex) newEmbds = ['art', 'art']\n",
    "    # (newEmbds): (N, 1024, 77)\n",
    "    newInputs = model.tokenizer(newPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    newEmbds = model.text_encoder(newInputs)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "    # 논문에서 제시한 closed-form solution은 W = [sum{(W^old)(c*_i)(c_i)^T}+lambda*(W^old)][sum{(c_i)(c_i)^T)}+lambda*(I)]^(-1).\n",
    "    # W의 첫번째 대괄호 부분이 m1, 두번째 대괄호가 m2. 즉 W = [m1][m2]^(-1).\n",
    "\n",
    "    # m1 = (W^old)[sum{(c*_i)(c_i)^T}+lambda*(I)].\n",
    "    # m1의 대괄호 부분이 m3. 즉 m1 = (W^old)[m3].\n",
    "\n",
    "    # m2 = [sum{(c_i)(c_i)^T)}+lambda*(I)].\n",
    "    # (m2): (1024, 1024)\n",
    "    m2 = (prevEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * eraseScale\n",
    "    m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "    # m3 = [sum{(c*_i)(c_i)^T}+lambda*(I)].\n",
    "    # (m3): (1024, 1024)\n",
    "    m3 = (newEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * eraseScale\n",
    "    m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "    # retainPrompts가 있다면 m2와 m3에 sum{(c_j)(c_j)^T} 추가\n",
    "    if retainPrompts:\n",
    "        # retainEmbds 생성. retainEmbds의 원소는 논문의 c*_j에 대응됨.\n",
    "        # (retainPrompts): (M,)\n",
    "        # (retainEmbds): (M, 1024, 77)\n",
    "        retainInputs = model.tokenizer(retainPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "        retainEmbds = model.text_encoder(retainInputs)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "        m2 += (retainEmbds @ retainEmbds.permute(0, 2, 1)).sum(0) * preserveScale\n",
    "        m3 += (retainEmbds @ retainEmbds.permute(0, 2, 1)).sum(0) * preserveScale\n",
    "\n",
    "    for targetLayer in targetLayers:\n",
    "        # (m1): (320, 1024)\n",
    "        # (targetLayer.weight): (320, 1024)\n",
    "        m1 = targetLayer.weight @ m3\n",
    "        targetLayer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "    \n",
    "    torch.save(model.unet.state_dict(), f\"model/{name}.pt\")\n",
    "\n",
    "    config = {\n",
    "        \"lamb\": lamb,\n",
    "        \"erase_scale\": eraseScale,\n",
    "        \"erase_scale\": preserveScale,\n",
    "        \"preserve_scale\": preserveScale,\n",
    "        \"withKey\": withKey\n",
    "    }\n",
    "    with open(f\"model/{name}.yaml\", 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8818b92820634981b91b3b76dd945f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.77it/s]\n",
      "100%|██████████| 100/100 [00:33<00:00,  2.99it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.29it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.28it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.28it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.16it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.15it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.18it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.24it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.24it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:32<00:00,  3.09it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "eraseArtistCount = 10\n",
    "lamb = 0.5\n",
    "eraseScale = 1\n",
    "preserveScale = 0.1\n",
    "withKey = False\n",
    "\n",
    "stepCount = 100\n",
    "guidanceScale = 7.5\n",
    "\n",
    "setSeed(2)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    df = pd.read_csv(\"data/artists1734_prompts.csv\")\n",
    "    artists = list(df.artist.unique())\n",
    "    random.shuffle(artists)\n",
    "\n",
    "    eraseArtists = artists[:eraseArtistCount]\n",
    "    retainArtists = artists[eraseArtistCount:]\n",
    "\n",
    "    prefixes = [\"\"]\n",
    "\n",
    "    prevPrompts = []\n",
    "    newPrompts = []\n",
    "    retainPrompts = []\n",
    "    for prefix in prefixes:\n",
    "        for eraseArtist in eraseArtists:\n",
    "            prevPrompts.append(prefix + eraseArtist)\n",
    "            newPrompts.append(prefix + \"art\")\n",
    "        for retainArtist in retainArtists:\n",
    "            retainPrompts.append(prefix + retainArtist)\n",
    "    retainPrompts = retainPrompts[:eraseArtistCount]\n",
    "    prompts = prevPrompts + retainPrompts\n",
    "    if len(retainPrompts) == 0: retainPrompts = None\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(device)\n",
    "\n",
    "    vae, tokenizer, textEncoder, unet = model.vae, model.tokenizer, model.text_encoder, model.unet\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "    seeds = [random.randint(0, 5000) for idx in range(len(prompts))]\n",
    "\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "        input = tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        embd = textEncoder(input)[0]\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        latent = torch.randn((1, unet.in_channels, 64, 64), generator=generator).to(device)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "        \n",
    "        scheduler.set_timesteps(stepCount)\n",
    "        for t in tqdm.tqdm(scheduler.timesteps):\n",
    "            latentInput = torch.cat([latent] * 2)\n",
    "            latentInput = scheduler.scale_model_input(latentInput, timestep=t)\n",
    "\n",
    "            noise = unet(latentInput, t, encoder_hidden_states=embd).sample\n",
    "            condNoise, uncondNoise = noise.chunk(2)\n",
    "            noise = uncondNoise + guidanceScale * (condNoise - uncondNoise)\n",
    "\n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "        latent /= 0.18215\n",
    "        image = vae.decode(latent).sample[0]\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(1, 2, 0)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image)\n",
    "        image.save(f\"image/default/{idx}.png\")\n",
    "\n",
    "    erasedModel = eraseModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, retainPrompts=retainPrompts,\n",
    "                            lamb=lamb, eraseScale=eraseScale, preserveScale=preserveScale, withKey=withKey)\n",
    "    unet = erasedModel.unet\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "        input = tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        embd = textEncoder(input)[0]\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        latent = torch.randn((1, unet.in_channels, 64, 64), generator=generator).to(device)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "        \n",
    "        scheduler.set_timesteps(stepCount)\n",
    "        for t in tqdm.tqdm(scheduler.timesteps):\n",
    "            latentInput = torch.cat([latent] * 2)\n",
    "            latentInput = scheduler.scale_model_input(latentInput, timestep=t)\n",
    "\n",
    "            noise = unet(latentInput, t, encoder_hidden_states=embd).sample\n",
    "            condNoise, uncondNoise = noise.chunk(2)\n",
    "            noise = uncondNoise + guidanceScale * (condNoise - uncondNoise)\n",
    "\n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "        latent /= 0.18215\n",
    "        image = vae.decode(latent).sample[0]\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(1, 2, 0)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image)\n",
    "        image.save(f\"image/erase/{idx}.png\")\n",
    "\n",
    "    df = pd.DataFrame({\"prompt\": prompts, \"seed\": seeds})\n",
    "    df.to_csv(\"image/df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anton Domenico Gabbiani'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "eraseArtistCount = 10\n",
    "lamb = 0.5\n",
    "eraseScale = 1\n",
    "preserveScale = 0.1\n",
    "withKey = False\n",
    "\n",
    "stepCount = 100\n",
    "guidanceScale = 7.5\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    df = pd.read_csv(\"data/artists1734_prompts.csv\")\n",
    "    artists = list(df.artist.unique())\n",
    "    random.shuffle(artists)\n",
    "\n",
    "    eraseArtists = artists[:eraseArtistCount]\n",
    "    retainArtists = artists[eraseArtistCount:]\n",
    "\n",
    "\n",
    "eraseArtists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss\n",
       "0   0.721\n",
       "1   0.723\n",
       "2   0.656\n",
       "3   0.688\n",
       "4   0.644\n",
       "5   0.665\n",
       "6   0.603\n",
       "7   0.759\n",
       "8   0.737\n",
       "9   0.632\n",
       "10  0.082\n",
       "11  0.182\n",
       "12  0.099\n",
       "13  0.252\n",
       "14  0.044\n",
       "15  0.347\n",
       "16  0.122\n",
       "17  0.019\n",
       "18  0.124\n",
       "19  0.121"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadImages(folderPath, scale=True):\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    images = []\n",
    "    for idx in range(len(os.listdir(folderPath))):\n",
    "        image = Image.open(f\"{folderPath}/{idx}.png\").convert('RGB')\n",
    "        image = preprocess(image)\n",
    "        images.append(image)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    if scale: images = images * 2 - 1\n",
    "    return images\n",
    "\n",
    "\n",
    "loss_fn = lpips.LPIPS(net='alex')\n",
    "\n",
    "images = loadImages(\"image/default\")\n",
    "erasedImages = loadImages(\"image/erase\")\n",
    "\n",
    "pd.DataFrame({\"loss\":loss_fn(images, erasedImages).squeeze().detach().numpy().round(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: tensor([0.3543, 0.3257, 0.3125, 0.3000, 0.3022, 0.3020, 0.3057, 0.3338, 0.3074,\n",
      "        0.3147, 0.3050, 0.2925, 0.3142, 0.3031, 0.2479, 0.2718, 0.3003, 0.3175,\n",
      "        0.3061, 0.2635])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images = loadImages(\"image/default\", False)\n",
    "inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    imageEmbds = outputs.image_embeds\n",
    "    textEmbds = outputs.text_embeds\n",
    "\n",
    "score = torch.nn.functional.cosine_similarity(imageEmbds, textEmbds)\n",
    "print(\"CLIP Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: tensor([0.1301, 0.1472, 0.1725, 0.1093, 0.2160, 0.1015, 0.2042, 0.2288, 0.1318,\n",
      "        0.2088, 0.3226, 0.3118, 0.2988, 0.3204, 0.2522, 0.2586, 0.2815, 0.3177,\n",
      "        0.3096, 0.2714])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images = loadImages(\"image/erase\", False)\n",
    "inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    imageEmbds = outputs.image_embeds\n",
    "    textEmbds = outputs.text_embeds\n",
    "\n",
    "score = torch.nn.functional.cosine_similarity(imageEmbds, textEmbds)\n",
    "print(\"CLIP Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageEmbds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_clip_score' from 'clip_score' (c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\clip_score\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclip_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_clip_score\n\u001b[0;32m      3\u001b[0m folderPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/default\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m imagePaths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolderPath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folderPath)))]\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_clip_score' from 'clip_score' (c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\clip_score\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from clip_score import get_clip_score\n",
    "\n",
    "folderPath = \"image/default\"\n",
    "imagePaths = [f\"{folderPath}/{idx}.png\" for idx in range(len(os.listdir(folderPath)))]\n",
    "scores = get_clip_score(imagePaths, prompts)\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Scores for text: '{prompt}'\")\n",
    "    for j, imagePath in enumerate(imagePaths):\n",
    "        print(f\"  {os.path.basename(imagePath)}: {scores[j][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b1f7fd1a1443af9af7fd5df613ec7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'eraseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-2-1-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 37\u001b[0m erasedModel \u001b[38;5;241m=\u001b[39m \u001b[43meraseModel\u001b[49m(model\u001b[38;5;241m=\u001b[39mmodel, prevPrompts\u001b[38;5;241m=\u001b[39mprevPrompts, newPrompts\u001b[38;5;241m=\u001b[39mnewPrompts, retainPrompts\u001b[38;5;241m=\u001b[39mretainPrompts,\n\u001b[0;32m     38\u001b[0m                          lamb\u001b[38;5;241m=\u001b[39mlamb, eraseScale\u001b[38;5;241m=\u001b[39meraseScale, preserveScale\u001b[38;5;241m=\u001b[39mpreserveScale, withKey\u001b[38;5;241m=\u001b[39mwithKey)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eraseModel' is not defined"
     ]
    }
   ],
   "source": [
    "#erase\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ################################################################################################################################\n",
    "\n",
    "    eraseArtistCount = 10\n",
    "    lamb = 0.5\n",
    "    eraseScale = 1\n",
    "    preserveScale = 0.1\n",
    "    withKey = False\n",
    "\n",
    "    ################################################################################################################################\n",
    "\n",
    "    df = pd.read_csv(\"data/artists1734_prompts.csv\")\n",
    "    artists = list(df.artist.unique())\n",
    "    random.shuffle(artists)\n",
    "\n",
    "    eraseArtists = artists[:eraseArtistCount]\n",
    "    retainArtists = artists[eraseArtistCount:]\n",
    "\n",
    "    prompts = [\"painting by \", \"art by \", \"artwork by \", \"picture by \", \"style of \", \"\"]\n",
    "    prompts = [\"\"]\n",
    "\n",
    "    prevPrompts = []\n",
    "    newPrompts = []\n",
    "    retainPrompts = []\n",
    "    for prompt in prompts:\n",
    "        for eraseArtist in eraseArtists:\n",
    "            prevPrompts.append(prompt + eraseArtist)\n",
    "            newPrompts.append(prompt + \"art\")\n",
    "        for retainArtist in retainArtists:\n",
    "            retainPrompts.append(prompt + retainArtist)\n",
    "    if len(retainPrompts) == 0: retainPrompts = None\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(device)\n",
    "    erasedModel = eraseModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, retainPrompts=retainPrompts,\n",
    "                             lamb=lamb, eraseScale=eraseScale, preserveScale=preserveScale, withKey=withKey)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdc14ef03b448a9b2b5cd8b0bcaabc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#moderate\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    concept_type = \"unsafe\"\n",
    "    concepts = \"violence, nudity, harm\"\n",
    "    guided_concepts = None\n",
    "  \n",
    "    concepts = [c.strip() for c in concepts.split(',')]\n",
    "\n",
    "    if concept_type == \"art\":\n",
    "        prompts = [\"painting by \", \"art by \", \"artwork by \", \"picture by \", \"style of \", \"\"]\n",
    "    elif concept_type == \"object\":\n",
    "        prompts = [\"image of \", \"photo of \", \"portrait of \", \"picture of \", \"painting of \", \"\"]\n",
    "    else:\n",
    "        prompts = [\"\"]\n",
    "\n",
    "    prevPrompts = []\n",
    "    for concept in concepts:\n",
    "        for prompt in prompts:\n",
    "            prevPrompts.append(prompt + concept)\n",
    "\n",
    "    newPrompts = []\n",
    "    if guided_concepts:\n",
    "        for concept in [guided_concepts] * len(concepts):\n",
    "            for prompt in prompts:\n",
    "                newPrompts.append(prompt + concept)\n",
    "    else:\n",
    "        newPrompts = [' '] * len(prevPrompts)\n",
    "    \n",
    "    lamb = 0.5\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "    model = model.to(device)\n",
    "    model = eraseModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, lamb=lamb,)\n",
    "\n",
    "    torch.save(model.unet.state_dict(), \"model/moderate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d693e139354ec890fb519a5a4a7b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e700068964c4cc4ab18af5919b18191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64da05c59dd4e5db409bc6499de8199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f65900c33614b4a831aedbe67c2cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4263af8050242c3994614d5d4f4e9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3e278d634b4d05bc6ea2c84cd3e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def debiasModel(model, prevPrompts, newPrompts, lamb=0.1, scale=0.1, withKey=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    prevPrompts에 해당하는 concept이 newPrompts에 해당하는 attribute에 대하여 debiasing된 모델 반환.\n",
    "\n",
    "    Parameters:\n",
    "        model (StableDiffusionPipeline): debias할 모델.\n",
    "        prevPrompts (List[str]): debias할 concept을 저장하고 있는 텍스트 리스트.\n",
    "        newPrompts (List[List[str]]): debias 대상 attribute를 저장하고 있는 텍스트 리스트의 리스트\n",
    "        lamb (float): 정규화 강도. Default: 0.1\n",
    "        scale (float): debiasing 강도. Default: 0.1\n",
    "        withKey (bool): key weight 업데이트 여부. Default: True\n",
    "\n",
    "    Returns:\n",
    "        model (StableDiffusionPipeline)\n",
    "    \"\"\"\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    # SD 모델 unet의 모든 cross-attention layer를 caLayers에 저장.\n",
    "    caLayers = []\n",
    "    for name, module in model.unet.named_modules():\n",
    "        # attn2가 cross-attention를 의미함.\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        caLayers.append(module)\n",
    "\n",
    "    # cross-attention layer의 value 부분을 valueLayers에 저장.\n",
    "    valueLayers = [layer.to_v for layer in caLayers]\n",
    "    # withKey 옵션이 켜져 있다면 key 부분도 추가함.\n",
    "    if withKey: valueLayers + [layer.to_k for layer in caLayers]\n",
    "\n",
    "    # 텍스트 리스트인 prevPrompts를 텍스트 임베딩인 prevEmbds으로 변환.\n",
    "    prevInputs = model.tokenizer(prevPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # prevEmbds는 (N, 768, 77) 형태이며 각 임베딩은 논문의 c_i에 대응됨. (N은 prevPrompts의 길이)\n",
    "    prevEmbds = model.text_encoder(prevInputs)[0].permute(0, 2, 1)\n",
    "\n",
    "    # eraseModel과 거의 같음.\n",
    "    # 다만 m3 = sum{(c_i^*)(c_i)^T}+lambda*(I)의 c_i^* 부분 대신 [(c_i)+sum{alpha/|(W_old)(a)|*|(W_old)(c_i)|*(a)}]이 사용됨.\n",
    "    # (a는 newPrompts의 원소인 newPrompt의 각 임배딩, alpha는 각 임베딩의 가중치)\n",
    "\n",
    "    # m2 = sum((c_i)(c_i)^T)}+lambda*(I)\n",
    "    m2 = (prevEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * scale\n",
    "    m2 += lamb * torch.eye(m2.shape[1], device=device)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    generator = torch.Generator(device=device)\n",
    "\n",
    "    # ratio 차이가 alpha에 반영되는 비율.\n",
    "    eta = 0.1\n",
    "    threshold = 0.05\n",
    "    alphas = [torch.zeros(len(t)) for t in newPrompts]\n",
    "    targetRatios = [torch.ones(len(t)) / len(t) for t in newPrompts]\n",
    "    # 각 prevPrompt에 해당하는 alpha의 업데이트가 필요한지 나타냄.\n",
    "    check = [0] * len(prevPrompts)\n",
    "    for _ in range(30):\n",
    "        for idx in range(len(prevPrompts)):\n",
    "\n",
    "            # alpha를 업데이트하지 않고 모두 0으로 바꾸어 value weight 업데이트가 일어나지 않도록 함.\n",
    "            if check[idx]: \n",
    "                alphas[idx] *= 0\n",
    "                continue\n",
    "        \n",
    "            prevPrompt = prevPrompts[idx]\n",
    "            newPrompt = newPrompts[idx]\n",
    "\n",
    "            # SD model로 prevPrompt에 해당하는 이미지 50개 생성\n",
    "            images = model(prevPrompt, num_images_per_prompt=50, num_inference_steps=20, generator=generator).images\n",
    "            \n",
    "            # score는 생성된 이미지 50개와 텍스트 리스트인 newPrompt 사이의 유사도 점수로 (50, M) 형태. (M은 newPrompt의 길이)\n",
    "            score = clip_model(**clip_processor(text=newPrompt, images=images, return_tensors=\"pt\", padding=True)).logits_per_image\n",
    "\n",
    "            # 각 이미지에 대해 가장 높은 유사도를 가진 점수를 1. 나머지를 0.으로 변환하여 각 이미지에 대해 평균을 냄.\n",
    "            # 즉 ratio는 prevPrompt로 생성된 이미지가 newPrompt의 각 prompt에 해당할 확률을 나타냄.\n",
    "            ratio = score.ge(score.max(1)[0].view(-1,1)).float().mean(0)\n",
    "            # 각 prompt에 해당할 확률이 동일하기를 원하기 때문에 targetRatio와의 차이가 반영 정도가 됨.\n",
    "            alpha = (eta * (targetRatios[idx] - ratio)).to(device)\n",
    "            alphas[idx] = alpha\n",
    "\n",
    "            # ratio와 targetratio와의 차이가 threshold 보다 작다면 더 이상 업데이트가 필요하지 않음.\n",
    "            if ratio.abs().max() < threshold: check[idx] = 1\n",
    "\n",
    "        # 모든 prompt에 대해서 업데이트가 필요하지 않다면 루프를 종료함.\n",
    "        if sum(check) == len(prevPrompts): break\n",
    "\n",
    "        # [(c_i)+sum{alpha/|(W_old)(a)|*|(W_old)(c_i)|*(a)}]\n",
    "        for valueLayer in valueLayers:\n",
    "            reEmbds = []\n",
    "            for idx in range(len(prevPrompts)):\n",
    "        \n",
    "                alpha = alphas[idx]\n",
    "                newPrompt = newPrompts[idx]\n",
    "                prevEmbd = prevEmbds[idx]\n",
    "\n",
    "                newInput = model.tokenizer(newPrompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "                # newEmbd은 (M, 768, 77) 형태의 텍스트 임베딩.\n",
    "                newEmbd = model.text_encoder(newInput)[0].permute(0, 2, 1)\n",
    "                # norm은 (M,) 형태의 벡터로 alpha/|(W_old)(a)|*|(W_old)(c_i)|를 나타냄.\n",
    "                norm = alpha / (valueLayer.weight @ newEmbd).norm(dim=[1,2]) * (valueLayer.weight @ prevEmbd).norm()\n",
    "                \n",
    "                # reEmbd은 (768, 77) 형태의 텍스트 임베딩으로 c_i + sum{alpha/|(W_old)(a)|*|(W_old)(c_i)|*(a)}를 나타냄.\n",
    "                reEmbd = prevEmbd + (norm.view(-1, 1, 1) * newEmbd).sum(0)\n",
    "                reEmbds.append(reEmbd.unsqueeze(0))\n",
    "            reEmbds = torch.concat(reEmbds, 0)\n",
    "\n",
    "            m3 = (reEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * scale\n",
    "            m3 += lamb * torch.eye(m3.shape[1], device=device)\n",
    "            # m1 = (W_old)[m3]\n",
    "            m1 = valueLayer.weight @ m3\n",
    "            valueLayer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "        \n",
    "    return model\n",
    "\n",
    "# debias\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    concepts = \"Doctor, Nurse, Carpenter\"\n",
    "    attributes = \"male, female\"\n",
    "\n",
    "    concepts = [c.strip() for c in concepts.split(',')]\n",
    "    attributes = [a.strip() for a in attributes.split(',')]\n",
    "\n",
    "    prompts = [\"image of \", \"photo of \", \"portrait of \", \"picture of \", \"\"]\n",
    "    \n",
    "    prevPrompts = []\n",
    "    newPrompts = []\n",
    "    for prompt in prompts:\n",
    "        for concept in concepts:\n",
    "            prevPrompts.append(prompt + concept)\n",
    "            newPrompt = []\n",
    "            for attribute in attributes:\n",
    "                newPrompt.append(prompt + attribute)\n",
    "            newPrompts.append(newPrompt)\n",
    "\n",
    "    lamb = 0.5\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "    model = model.to(device)\n",
    "    model = debiasModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, lamb=lamb,)\n",
    "\n",
    "    torch.save(model.unet.state_dict(), \"model/debias.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a4f9ad3d9f4bf2b555aadaea5cb98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n",
      "100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generateImage(promptPath, modelVersion=\"1.4\", sampleCount=10, imageSize=(512, 512), stepCount=100, guidanceScale=7.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 prompt 파일에 해당하는 이미지를 생성하여 \"image\" 폴더에 저장.\n",
    "\n",
    "    Parameters:\n",
    "        promptPath (str): prompt 파일이 저장된 경로.\n",
    "        modelVersion (str): 이미지 생성시 사용되는 모델의 버전. Default: \"1.4\"\n",
    "        sampleCount (int): 각 prompt마다 생성할 이미지 개수. Default: 10\n",
    "        imageSize (tuple): 생성할 이미지 크기. Default: (512, 512)\n",
    "        stepCount (int): sampling 과정의 inference step 수. Default: 100\n",
    "        guidanceScale (float): classifier-free guidance 강도. Default: 7.5\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if modelVersion == \"1.4\": modelVersion = \"CompVis/stable-diffusion-v1-4\"\n",
    "    elif modelVersion == \"2.1\": modelVersion = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # modelVersion에 따라 모델 선택.\n",
    "    model = StableDiffusionPipeline.from_pretrained(modelVersion).to(device)\n",
    "    vae, tokenizer, textEncoder, unet = model.vae, model.tokenizer, model.text_encoder, model.unet\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "    promptDf = pd.read_csv(promptPath, index_col=0)\n",
    "    # promptDf의 각 행 불러오기.\n",
    "    for _, row in promptDf.iterrows():\n",
    "\n",
    "        # B는 생성할 이미지 개수.\n",
    "        B = sampleCount\n",
    "        H, W = imageSize\n",
    "\n",
    "        # classifier-free guidance를 위해 conditional과 uncoditional prompt가 필요함.\n",
    "        prompts = [row.prompt] * B + [\"\"] * B\n",
    "        inputs = tokenizer(prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        # embds는 (2*B, 768, 77) 형태를 가지는 텍스트 임베딩.\n",
    "        embds = textEncoder(inputs)[0]\n",
    "\n",
    "        seed = row.evaluation_seed\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        # 주어진 시드를 가지는 B개의 latent 생성.\n",
    "        latents = torch.randn((B, unet.in_channels, H//8, W//8), generator=generator).to(device)\n",
    "        latents *= scheduler.init_noise_sigma\n",
    "        \n",
    "        # inference step 수를 설정.\n",
    "        scheduler.set_timesteps(stepCount)\n",
    "        for t in tqdm.tqdm(scheduler.timesteps):\n",
    "\n",
    "            # classifier-free guidance를 위해 uncoditional latent를 추가함.\n",
    "            latentInputs = torch.cat([latents]*2)\n",
    "            # latentInputs를 현재 timestep에 맞게 조정.\n",
    "            latentInputs = scheduler.scale_model_input(latentInputs, timestep=t)\n",
    "\n",
    "            # latent와 텍스트 임베딩을 사용하여 노이즈 생성.\n",
    "            noises = unet(latentInputs, t, encoder_hidden_states=embds).sample\n",
    "            # 앞쪽 B개의 노이즈는 conditional과, 뒤쪽 B개의 노이즈는 uncoditional.\n",
    "            condNoises, uncondNoises = noises.chunk(2)\n",
    "            # classifier-free guidance 적용.\n",
    "            noises = uncondNoises + guidanceScale * (condNoises - uncondNoises)\n",
    "\n",
    "            # 다음 latent 생성.\n",
    "            latents = scheduler.step(noises, t, latents).prev_sample\n",
    "\n",
    "        latents /= 0.18215\n",
    "        # 최종 latent를 decoding하여 이미지 생성.\n",
    "        images = vae.decode(latents).sample\n",
    "\n",
    "        # images의 값을 [0,1]로 제한, 형태를 (B, C, H, W)에서 (B, H, W, C)로 변환.\n",
    "        images = ((images + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        # numpy array 사용을 위해 cpu로 이동.\n",
    "        images = images.detach().cpu().numpy()\n",
    "        # [0,1] 범위의 float를 [0,255] 범위의 uint8로 변환.\n",
    "        images = (images * 255).round().astype(\"uint8\")\n",
    "        # numpy array를 PIL Image로 변환.\n",
    "        images = [Image.fromarray(image) for image in images]\n",
    "        # images에 저장된 B개의 image 저장\n",
    "        for idx, image in enumerate(images):\n",
    "            image.save(f\"image/{row.case_number}_{idx}.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    promptPath = \"data/test_prompts.csv\"\n",
    "    generateImage(promptPath, sampleCount=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generateImages(model, prompts, \n",
    "                   stepCount=100, guidanceScale=7.5,\n",
    "                   path=\"image/erase\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 prompts에 해당하는 이미지 반환.\n",
    "\n",
    "    Parameters:\n",
    "        model (StableDiffusionPipeline): 이미지 생성시 사용할 모델\n",
    "        prompts (List[str]): 이미지 생성시 사용할 프롬프트 string 리스트.\n",
    "        sampleCount (int): prompt마다 생성할 이미지 개수. Default: 10\n",
    "        imageSize (tuple): 생성할 이미지 크기. Default: (512, 512)\n",
    "        stepCount (int): sampling 과정의 inference step 수. Default: 100\n",
    "        guidanceScale (float): classifier-free guidance 강도. Default: 7.5\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    device = model.device\n",
    "    vae, tokenizer, textEncoder, unet = model.vae, model.tokenizer, model.text_encoder, model.unet\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "    seeds = [random.randint(0, 5000) for idx in range(len(prompts))]\n",
    "    paths = [f\"{path}/{idx}.png\" for idx in range(len(prompts))]\n",
    "\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "\n",
    "        B = 1\n",
    "        H, W = (512, 512)\n",
    "\n",
    "        prompt = [prompt] * B + [\"\"] * B\n",
    "        input = tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        embd = textEncoder(input)[0]\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        latent = torch.randn((B, unet.in_channels, H//8, W//8), generator=generator).to(device)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "        \n",
    "        scheduler.set_timesteps(stepCount)\n",
    "        for t in tqdm.tqdm(scheduler.timesteps):\n",
    "            latentInput = torch.cat([latent] * 2)\n",
    "            latentInput = scheduler.scale_model_input(latentInput, timestep=t)\n",
    "\n",
    "            noise = unet(latentInput, t, encoder_hidden_states=embd).sample\n",
    "            condNoise, uncondNoise = noise.chunk(2)\n",
    "            noise = uncondNoise + guidanceScale * (condNoise - uncondNoise)\n",
    "\n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "        latent /= 0.18215\n",
    "        image = vae.decode(latent).sample\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image[0])\n",
    "        image.save(paths[idx])\n",
    "\n",
    "    df = pd.DataFrame({\"prompt\": prompts, \"seed\": seeds, })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

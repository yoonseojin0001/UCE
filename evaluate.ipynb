{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lpips\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>LPIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.5972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.6957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.4844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.3537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.3085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.2886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.1787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist   LPIPS\n",
       "0        0  0.5347\n",
       "1        1  0.5228\n",
       "2        2  0.4738\n",
       "3        3  0.6550\n",
       "4        4  0.5408\n",
       "5        5  0.6221\n",
       "6        6  0.5972\n",
       "7        7  0.5459\n",
       "8        8  0.6957\n",
       "9        9  0.4844\n",
       "10      10  0.1643\n",
       "11      11  0.1899\n",
       "12      12  0.2083\n",
       "13      13  0.3537\n",
       "14      14  0.3085\n",
       "15      15  0.1827\n",
       "16      16  0.1521\n",
       "17      17  0.2886\n",
       "18      18  0.3269\n",
       "19      19  0.1787"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name1 = \"default_1\"\n",
    "model_name2 = \"artist10erase_1\"\n",
    "\n",
    "lpips_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "images1 = []\n",
    "images2 = []\n",
    "\n",
    "artist_index = []\n",
    "count_index = []\n",
    "for idx in range(20):\n",
    "    for i in range(10):\n",
    "        image1 = lpips_preprocess(Image.open(f\"image/{model_name1}/{idx}_{i}.png\").convert(\"RGB\")) * 2 - 1\n",
    "        images1.append(image1)\n",
    "        image2 = lpips_preprocess(Image.open(f\"image/{model_name2}/{idx}_{i}.png\").convert(\"RGB\")) * 2 - 1\n",
    "        images2.append(image2)\n",
    "        artist_index.append(idx)\n",
    "        count_index.append(i)\n",
    "images1 = torch.stack(images1)\n",
    "images2 = torch.stack(images2)\n",
    "\n",
    "\n",
    "loss_function = lpips.LPIPS(net='alex')\n",
    "lpips_score = loss_function(images1, images2).squeeze().detach().numpy().round(3)\n",
    "df = pd.DataFrame({\"artist\": artist_index, \"index\": count_index, \"LPIPS\": lpips_score})\n",
    "df.groupby(\"artist\")[\"LPIPS\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>CLIP 1</th>\n",
       "      <th>CLIP 2</th>\n",
       "      <th>CLIP diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3185</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.0960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.0369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3547</td>\n",
       "      <td>0.3137</td>\n",
       "      <td>0.0410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.2085</td>\n",
       "      <td>0.0934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3413</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>0.1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.1257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3112</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.0962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>0.2412</td>\n",
       "      <td>0.1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.2856</td>\n",
       "      <td>0.1808</td>\n",
       "      <td>0.1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.3227</td>\n",
       "      <td>0.2735</td>\n",
       "      <td>0.0514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.2496</td>\n",
       "      <td>0.2579</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>0.2817</td>\n",
       "      <td>0.0136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.2808</td>\n",
       "      <td>0.2599</td>\n",
       "      <td>0.0215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.3138</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.3629</td>\n",
       "      <td>0.3620</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.3359</td>\n",
       "      <td>0.3385</td>\n",
       "      <td>0.0108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.2514</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.0081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist  CLIP 1  CLIP 2  CLIP diff\n",
       "0        0  0.3185  0.2225     0.0960\n",
       "1        1  0.2028  0.1873     0.0369\n",
       "2        2  0.3547  0.3137     0.0410\n",
       "3        3  0.3019  0.2085     0.0934\n",
       "4        4  0.3413  0.2384     0.1029\n",
       "5        5  0.3534  0.2277     0.1257\n",
       "6        6  0.3112  0.2150     0.0962\n",
       "7        7  0.3460  0.2412     0.1048\n",
       "8        8  0.2856  0.1808     0.1048\n",
       "9        9  0.3227  0.2735     0.0514\n",
       "10      10  0.2981  0.2941     0.0066\n",
       "11      11  0.2496  0.2579     0.0089\n",
       "12      12  0.2757  0.2817     0.0136\n",
       "13      13  0.2808  0.2599     0.0215\n",
       "14      14  0.3055  0.3064     0.0161\n",
       "15      15  0.3138  0.3103     0.0103\n",
       "16      16  0.3184  0.3105     0.0107\n",
       "17      17  0.3629  0.3620     0.0111\n",
       "18      18  0.3359  0.3385     0.0108\n",
       "19      19  0.2514  0.2567     0.0081"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "images1 = []\n",
    "images2 = []\n",
    "index = []\n",
    "for idx in range(20):\n",
    "    for i in range(10):\n",
    "        image1 = clip_preprocess(Image.open(f\"image/{model_name1}/{idx}_{i}.png\").convert(\"RGB\"))\n",
    "        images1.append(image1)\n",
    "        image2 = clip_preprocess(Image.open(f\"image/{model_name2}/{idx}_{i}.png\").convert(\"RGB\"))\n",
    "        images2.append(image2)\n",
    "        index.append(f\"{idx}_{i}\")\n",
    "images1 = torch.stack(images1)\n",
    "images2 = torch.stack(images2)\n",
    "\n",
    "with open(f\"data/{model_name2}.yaml\", 'r', encoding='utf-8') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "prompts = data[\"prompts\"][:20]\n",
    "total_prompts = []\n",
    "for prompt in prompts:\n",
    "    total_prompts += [prompt] * 10\n",
    "\n",
    "inputs = processor(text=total_prompts, images=images1, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "outputs = model(**inputs)\n",
    "image_embds1 = outputs.image_embeds\n",
    "\n",
    "inputs = processor(text=total_prompts, images=images2, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "outputs = model(**inputs)\n",
    "image_embds2 = outputs.image_embeds\n",
    "\n",
    "text_embds = outputs.text_embeds\n",
    "\n",
    "clip_score1 = torch.nn.functional.cosine_similarity(image_embds1, text_embds).numpy().round(3)\n",
    "clip_score2 = torch.nn.functional.cosine_similarity(image_embds2, text_embds).numpy().round(3)\n",
    "\n",
    "df = pd.DataFrame({\"artist\": artist_index, \"index\": count_index, \"CLIP 1\": clip_score1, \"CLIP 2\": clip_score2,\n",
    "                    \"CLIP diff\": abs(clip_score1 - clip_score2)})\n",
    "df.groupby(\"artist\")[[\"CLIP 1\", \"CLIP 2\", \"CLIP diff\"]].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

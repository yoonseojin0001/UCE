{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "artist_idx = 0\n",
    "with open(f\"data/artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)\n",
    "artist = artist[artist_idx]\n",
    "\n",
    "#\n",
    "sample_count = 10\n",
    "prompt_idx = 0\n",
    "prompt = [f\"{artist}\", f\"An image in the style of {artist}\", f\"An image depicting {artist}\"]\n",
    "\n",
    "image = []\n",
    "for _ in range(sample_count):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = [prompt[prompt_idx]] + [\"\"]\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((1, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image.append(pipeline.vae.decode(latent).sample.detach().cpu()[0])\n",
    "\n",
    "image = np.stack(image)\n",
    "image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "image = (image * 255).round().astype(\"uint8\")\n",
    "image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "#\n",
    "print(f\"#\\nPrompt: {prompt[prompt_idx]}\\nScore:\")\n",
    "for p in prompt:\n",
    "    input = processor(text=[p] * sample_count, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    \n",
    "    score = torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy()\n",
    "    for s in score:\n",
    "        print(f\"{s:.3f}\", end=\" \")\n",
    "    print(f\"\\tMean: {score.mean():.3f}\\tStd: {score.std(ddof=1):.3f}\\t{p}\")\n",
    "\n",
    "for i in image:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d701ec508f745608c414da65b8954c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "Prompt: Pablo Picasso\n",
      "Score:\n",
      "0.276 0.259 0.336 0.242 0.242 0.244 0.301 0.274 0.285 0.272 \tMean: 0.273\tStd: 0.030\tPablo Picasso\n",
      "0.253 0.216 0.284 0.199 0.258 0.245 0.292 0.257 0.229 0.288 \tMean: 0.252\tStd: 0.031\tAn image in the style of Pablo Picasso\n",
      "0.291 0.250 0.345 0.238 0.248 0.249 0.303 0.295 0.279 0.277 \tMean: 0.277\tStd: 0.033\tAn image depicting Pablo Picasso\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "pipeline.unet.load_state_dict(torch.load(f\"model/erase10.pth\"))\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "with open(f\"data/artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)\n",
    "a = artist[0]\n",
    "a = \"Pablo Picasso\"\n",
    "\n",
    "#\n",
    "sample_count = 10\n",
    "prompt = [f\"{a}\"] * sample_count\n",
    "# prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "# prompt = [f\"An image depicting {a}\"] * sample_count\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(100)\n",
    "\n",
    "cfg_prompt = prompt + [\"\"] * sample_count\n",
    "token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "for t in scheduler.timesteps:\n",
    "    latent_input = torch.cat([latent] * 2)\n",
    "    latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "    noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "    cond_noise, uncond_noise = noise.chunk(2)\n",
    "    noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "    latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "latent /= 0.18215\n",
    "image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "image = (image * 255).round().astype(\"uint8\")\n",
    "image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "print(f\"#\\nPrompt: {prompt[0]}\\nScore:\")\n",
    "prompt = [f\"{a}\", f\"An image in the style of {a}\", f\"An image depicting {a}\"]\n",
    "for p in prompt:\n",
    "    input = processor(text=[p] * sample_count, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy()\n",
    "    \n",
    "    for s in score:\n",
    "        print(f\"{s:.3f}\", end=\" \")\n",
    "    print(f\"\\tMean: {score.mean():.3f}\\tStd: {score.std(ddof=1):.3f}\\t{p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "with open(f\"data/artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)\n",
    "\n",
    "valid_artist = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"]\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"]\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((1, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().item()\n",
    "    print(score)\n",
    "\n",
    "    if score > 0.29:\n",
    "        valid_artist.append(a)\n",
    "\n",
    "#\n",
    "# with open(f\"data/valid_artist.yaml\", 'w') as file:\n",
    "#     yaml.dump(valid_artist, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = UNet2DConditionModel.from_pretrained(version, subfolder=\"unet\").to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(version, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(version, subfolder=\"text_encoder\").to(device)\n",
    "\n",
    "#\n",
    "with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)\n",
    "random.shuffle(artist)\n",
    "with open(f\"data/valid_artist.yaml\", 'w') as file:\n",
    "    yaml.dump(artist, file)\n",
    "# artist = artist[:1043]\n",
    "\n",
    "artist_count = 3\n",
    "prompt = [\"An image in the style of \" + a for a in artist]\n",
    "prev_prompt = prompt[:artist_count]\n",
    "new_prompt = [\"art\"] * artist_count\n",
    "retain_prompt = prompt[artist_count:]\n",
    "\n",
    "#\n",
    "lamb = 0.5\n",
    "erase_scale = 1\n",
    "preserve_scale = 0.1\n",
    "with_key = True\n",
    "\n",
    "ca_layer = []\n",
    "for n, module in unet.named_modules():\n",
    "    if n[-5:] != \"attn2\": continue\n",
    "    ca_layer.append(module)\n",
    "\n",
    "value_layer = [layer.to_v for layer in ca_layer]\n",
    "target_layer = value_layer\n",
    "\n",
    "if with_key:\n",
    "    key_layer = [layer.to_k for layer in ca_layer]\n",
    "    target_layer += key_layer\n",
    "\n",
    "prev_token = tokenizer(prev_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "prev_embd = text_encoder(prev_token)[0].permute(0, 2, 1)\n",
    "\n",
    "new_token = tokenizer(new_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "new_embd = text_encoder(new_token)[0].permute(0, 2, 1)\n",
    "\n",
    "m2 = (prev_embd @ prev_embd.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "m3 = (new_embd @ prev_embd.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "if retain_prompt:\n",
    "    retain_token = tokenizer(retain_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    retain_embd = text_encoder(retain_token)[0].permute(0, 2, 1)\n",
    "\n",
    "    m2 += (retain_embd @ retain_embd.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "    m3 += (retain_embd @ retain_embd.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "\n",
    "for layer in target_layer:\n",
    "    m1 = layer.weight @ m3\n",
    "    layer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "\n",
    "torch.save(unet.state_dict(), f\"model/erase10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc90c819b7c144b1a2ab5223cdfd681d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.268, 0.3222, 0.2948, 0.2825, 0.2841], [0.3491, 0.3458, 0.3496, 0.3346, 0.3357], [0.3214, 0.3216, 0.3018, 0.307, 0.3143], [0.2991, 0.2867, 0.3015, 0.3022, 0.2997], [0.3223, 0.3367, 0.3112, 0.3318, 0.3412], [0.3484, 0.3328, 0.3525, 0.3367, 0.3284]]\n",
      "[[0.1604, 0.1792, 0.2295, 0.2505, 0.1755], [0.1709, 0.2236, 0.1677, 0.1855, 0.2037], [0.1941, 0.2267, 0.2006, 0.1538, 0.1971], [0.2775, 0.2895, 0.2663, 0.2703, 0.2929], [0.3436, 0.3392, 0.3344, 0.3136, 0.3237], [0.3442, 0.3432, 0.3425, 0.3438, 0.3198]]\n",
      "\n",
      "0.290 (0.020) -> 0.199 (0.039)    diff: 0.091\n",
      "0.343 (0.007) -> 0.190 (0.023)    diff: 0.153\n",
      "0.313 (0.009) -> 0.194 (0.026)    diff: 0.119\n",
      "\n",
      "0.315 (0.012) -> 0.195 (0.012)    diff: 0.121\n",
      "\n",
      "0.298 (0.006) -> 0.279 (0.012)    diff: 0.019\n",
      "0.329 (0.012) -> 0.331 (0.012)    diff: -0.002\n",
      "0.340 (0.010) -> 0.339 (0.011)    diff: 0.001\n",
      "\n",
      "0.322 (0.010) -> 0.316 (0.010)    diff: 0.006\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "artist_count = 3\n",
    "sample_count = 5\n",
    "\n",
    "with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)[:artist_count * 2]\n",
    "\n",
    "#\n",
    "default_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    default_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "pipeline.unet.load_state_dict(torch.load(f\"model/erase10.pth\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "erased_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    erased_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "print(default_score)\n",
    "print(erased_score)\n",
    "print()\n",
    "default_std = []\n",
    "erased_std = []\n",
    "for i in range(artist_count):\n",
    "    d_mean = np.mean(default_score[i])\n",
    "    d_std = np.std(default_score[i], ddof=1)\n",
    "    default_std.append(d_std)\n",
    "    e_mean = np.mean(erased_score[i])\n",
    "    e_std = np.std(erased_score[i], ddof=1)\n",
    "    erased_std.append(e_std)\n",
    "    print(f\"{d_mean:.3f} ({d_std:.3f}) -> {e_mean:.3f} ({e_std:.3f})    diff: {d_mean - e_mean:.3f}\")\n",
    "print()\n",
    "d_mean = np.mean(default_score[:artist_count])\n",
    "e_mean = np.mean(erased_score[:artist_count])\n",
    "print(f\"{d_mean:.3f} ({np.mean(default_std):.3f}) -> {e_mean:.3f} ({np.mean(default_std):.3f})    diff: {d_mean - e_mean:.3f}\")\n",
    "print()\n",
    "default_std = []\n",
    "erased_std = []\n",
    "for i in range(artist_count, artist_count * 2):\n",
    "    d_mean = np.mean(default_score[i])\n",
    "    d_std = np.std(default_score[i], ddof=1)\n",
    "    default_std.append(d_std)\n",
    "    e_mean = np.mean(erased_score[i])\n",
    "    e_std = np.std(erased_score[i], ddof=1)\n",
    "    erased_std.append(e_std)\n",
    "    print(f\"{d_mean:.3f} ({d_std:.3f}) -> {e_mean:.3f} ({e_std:.3f})    diff: {d_mean - e_mean:.3f}\")\n",
    "print()\n",
    "d_mean = np.mean(default_score[artist_count:])\n",
    "e_mean = np.mean(erased_score[artist_count:])\n",
    "print(f\"{d_mean:.3f} ({np.mean(default_std):.3f}) -> {e_mean:.3f} ({np.mean(default_std):.3f})    diff: {d_mean - e_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfa398dce8b4055a2ddc30a8d4adce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def eraseModel(model, prevPrompts, newPrompts, retainPrompts=None, \n",
    "               lamb=0.1, eraseScale=0.1, preserveScale=0.1, withKey=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    prevPrompts에 해당하는 개념을 newPrompts로 편집한 모델 반환.\n",
    "\n",
    "    Parameters:\n",
    "        model (StableDiffusionPipeline): 편집할 모델.\n",
    "        prevPrompts (List[str]): 원래 개념이 저장된 string 리스트.\n",
    "        newPrompts (List[str]): 대체할 개념이 저장된 string 리스트.\n",
    "        retainPrompts (List[str] | None): 보존할 개념이 저장된 string 리스트. Default: None\n",
    "        lamb (float): 정규화 강도. Default: 0.1\n",
    "        eraseScale (float): 개념 편집 강도. Default: 0.1\n",
    "        preserveScale (float): 개념 보존 강도. Default: 0.1\n",
    "        withKey (bool): key 가중치 업데이트 여부. Default: True\n",
    "\n",
    "    Returns:\n",
    "        model (StableDiffusionPipeline)\n",
    "    \"\"\"\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    # 주어진 모델 unet의 cross-attention layer를 caLayers에 저장.\n",
    "    caLayers = []\n",
    "    for name, module in model.unet.named_modules():\n",
    "        # attn2로 끝나는 모듈이 cross-attention layer.\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        caLayers.append(module)\n",
    "\n",
    "    # value projection layer를 targetLayers에 저장. 논문의 W^old에 해당하는 부분.\n",
    "    # (value projection layer): Linear(in_features=1024, out_features=320, bias=False)\n",
    "    # \"stabilityai/stable-diffusion-2-1-base\" 모델이 기준. \"CompVis/stable-diffusion-v1-4\" 모델은 1024 대신 768을 사용.\n",
    "    valueLayers = [layer.to_v for layer in caLayers]\n",
    "    targetLayers = valueLayers\n",
    "\n",
    "    # withKey=True라면 key projection layer도 추가.\n",
    "    if withKey: \n",
    "        # (key projection layer): Linear(in_features=1024, out_features=320, bias=False)\n",
    "        keyLayers = [layer.to_k for layer in caLayers]\n",
    "        targetLayers += keyLayers\n",
    "    \n",
    "    # 텍스트 prevPrompts를 텍스트 임베딩 prevEmbds으로 변환. prevEmbds의 원소는 논문의 c_i에 대응됨.\n",
    "    # (prevPrompts): (N,)\n",
    "    # ex) N = 2; prevPrompts = [\"red rose\", \"blue rose\"]\n",
    "    # (prevEmbds): (N, 1024, 77)\n",
    "    prevInputs = model.tokenizer(prevPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prevEmbds = model.text_encoder(prevInputs)[0].permute(0, 2, 1)\n",
    "\n",
    "    # 마찬가지로 newEmbds 생성. newEmbds의 원소는 논문의 c*_i에 대응됨.\n",
    "    # (newPrompts): (N,)\n",
    "    # (newEmbds): (N, 1024, 77)\n",
    "    newInputs = model.tokenizer(newPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    newEmbds = model.text_encoder(newInputs)[0].permute(0, 2, 1)\n",
    "\n",
    "    # 논문에서 제시한 closed-form solution은 W = [sum{(W^old)(c*_i)(c_i)^T}+lambda*(W^old)][sum{(c_i)(c_i)^T)}+lambda*(I)]^(-1).\n",
    "    # W의 첫번째 대괄호 부분이 m1, 두번째 대괄호가 m2. 즉 W = [m1][m2]^(-1).\n",
    "\n",
    "    # m1 = (W^old)[sum{(c*_i)(c_i)^T}+lambda*(I)].\n",
    "    # m1의 대괄호 부분이 m3. 즉 m1 = (W^old)[m3].\n",
    "\n",
    "    # m2 = [sum{(c_i)(c_i)^T)}+lambda*(I)].\n",
    "    # (m2): (1024, 1024)\n",
    "    m2 = (prevEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * eraseScale\n",
    "    m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "    # m3 = [sum{(c*_i)(c_i)^T}+lambda*(I)].\n",
    "    # (m3): (1024, 1024)\n",
    "    m3 = (newEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * eraseScale\n",
    "    m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "    # retainPrompts가 있다면 m2와 m3에 sum{(c_j)(c_j)^T} 추가\n",
    "    if retainPrompts:\n",
    "        # retainEmbds 생성. retainEmbds의 원소는 논문의 c*_j에 대응됨.\n",
    "        # (retainPrompts): (M,)\n",
    "        # (retainEmbds): (M, 1024, 77)\n",
    "        retainInputs = model.tokenizer(retainPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        retainEmbds = model.text_encoder(retainInputs)[0].permute(0, 2, 1)\n",
    "\n",
    "        m2 += (retainEmbds @ retainEmbds.permute(0, 2, 1)).sum(0) * preserveScale\n",
    "        m3 += (retainEmbds @ retainEmbds.permute(0, 2, 1)).sum(0) * preserveScale\n",
    "\n",
    "    for targetLayer in targetLayers:\n",
    "        # (m1): (320, 1024)\n",
    "        # (targetLayer.weight): (320, 1024)\n",
    "        m1 = targetLayer.weight @ m3\n",
    "        targetLayer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    concept_type = \"art\"\n",
    "    concepts = \"Kelly Mckernan, Sarah Anderson\"\n",
    "    guided_concepts = \"art\"\n",
    "  \n",
    "    concepts = [c.strip() for c in concepts.split(',')]\n",
    "\n",
    "    if concept_type == \"art\":\n",
    "        prompts = [\"painting by \", \"art by \", \"artwork by \", \"picture by \", \"style of \", \"\"]\n",
    "    elif concept_type == \"object\":\n",
    "        prompts = [\"image of \", \"photo of \", \"portrait of \", \"picture of \", \"painting of \", \"\"]\n",
    "    else:\n",
    "        prompts = [\"\"]\n",
    "\n",
    "    prevPrompts = []\n",
    "    for concept in concepts:\n",
    "        for prompt in prompts:\n",
    "            prevPrompts.append(prompt + concept)\n",
    "\n",
    "    newPrompts = []\n",
    "    if guided_concepts:\n",
    "        for concept in [guided_concepts] * len(concepts):\n",
    "            for prompt in prompts:\n",
    "                newPrompts.append(prompt + concept)\n",
    "    else:\n",
    "        newPrompts = [' '] * len(prevPrompts)\n",
    "    \n",
    "    lamb = 0.5\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "    model = model.to(device)\n",
    "    model = eraseModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, lamb=lamb,)\n",
    "\n",
    "    torch.save(model.unet.state_dict(), \"model/erase.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdc14ef03b448a9b2b5cd8b0bcaabc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#moderate\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    concept_type = \"unsafe\"\n",
    "    concepts = \"violence, nudity, harm\"\n",
    "    guided_concepts = None\n",
    "  \n",
    "    concepts = [c.strip() for c in concepts.split(',')]\n",
    "\n",
    "    if concept_type == \"art\":\n",
    "        prompts = [\"painting by \", \"art by \", \"artwork by \", \"picture by \", \"style of \", \"\"]\n",
    "    elif concept_type == \"object\":\n",
    "        prompts = [\"image of \", \"photo of \", \"portrait of \", \"picture of \", \"painting of \", \"\"]\n",
    "    else:\n",
    "        prompts = [\"\"]\n",
    "\n",
    "    prevPrompts = []\n",
    "    for concept in concepts:\n",
    "        for prompt in prompts:\n",
    "            prevPrompts.append(prompt + concept)\n",
    "\n",
    "    newPrompts = []\n",
    "    if guided_concepts:\n",
    "        for concept in [guided_concepts] * len(concepts):\n",
    "            for prompt in prompts:\n",
    "                newPrompts.append(prompt + concept)\n",
    "    else:\n",
    "        newPrompts = [' '] * len(prevPrompts)\n",
    "    \n",
    "    lamb = 0.5\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "    model = model.to(device)\n",
    "    model = eraseModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, lamb=lamb,)\n",
    "\n",
    "    torch.save(model.unet.state_dict(), \"model/moderate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d693e139354ec890fb519a5a4a7b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e700068964c4cc4ab18af5919b18191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64da05c59dd4e5db409bc6499de8199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f65900c33614b4a831aedbe67c2cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4263af8050242c3994614d5d4f4e9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3e278d634b4d05bc6ea2c84cd3e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def debiasModel(model, prevPrompts, newPrompts, lamb=0.1, scale=0.1, withKey=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    prevPrompts에 해당하는 concept이 newPrompts에 해당하는 attribute에 대하여 debiasing된 모델 반환.\n",
    "\n",
    "    Parameters:\n",
    "        model (StableDiffusionPipeline): debias할 모델.\n",
    "        prevPrompts (List[str]): debias할 concept을 저장하고 있는 텍스트 리스트.\n",
    "        newPrompts (List[List[str]]): debias 대상 attribute를 저장하고 있는 텍스트 리스트의 리스트\n",
    "        lamb (float): 정규화 강도. Default: 0.1\n",
    "        scale (float): debiasing 강도. Default: 0.1\n",
    "        withKey (bool): key weight 업데이트 여부. Default: True\n",
    "\n",
    "    Returns:\n",
    "        model (StableDiffusionPipeline)\n",
    "    \"\"\"\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    # SD 모델 unet의 모든 cross-attention layer를 caLayers에 저장.\n",
    "    caLayers = []\n",
    "    for name, module in model.unet.named_modules():\n",
    "        # attn2가 cross-attention를 의미함.\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        caLayers.append(module)\n",
    "\n",
    "    # cross-attention layer의 value 부분을 valueLayers에 저장.\n",
    "    valueLayers = [layer.to_v for layer in caLayers]\n",
    "    # withKey 옵션이 켜져 있다면 key 부분도 추가함.\n",
    "    if withKey: valueLayers + [layer.to_k for layer in caLayers]\n",
    "\n",
    "    # 텍스트 리스트인 prevPrompts를 텍스트 임베딩인 prevEmbds으로 변환.\n",
    "    prevInputs = model.tokenizer(prevPrompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # prevEmbds는 (N, 768, 77) 형태이며 각 임베딩은 논문의 c_i에 대응됨. (N은 prevPrompts의 길이)\n",
    "    prevEmbds = model.text_encoder(prevInputs)[0].permute(0, 2, 1)\n",
    "\n",
    "    # eraseModel과 거의 같음.\n",
    "    # 다만 m3 = sum{(c_i^*)(c_i)^T}+lambda*(I)의 c_i^* 부분 대신 [(c_i)+sum{alpha/|(W_old)(a)|*|(W_old)(c_i)|*(a)}]이 사용됨.\n",
    "    # (a는 newPrompts의 원소인 newPrompt의 각 임배딩, alpha는 각 임베딩의 가중치)\n",
    "\n",
    "    # m2 = sum((c_i)(c_i)^T)}+lambda*(I)\n",
    "    m2 = (prevEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * scale\n",
    "    m2 += lamb * torch.eye(m2.shape[1], device=device)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    generator = torch.Generator(device=device)\n",
    "\n",
    "    # ratio 차이가 alpha에 반영되는 비율.\n",
    "    eta = 0.1\n",
    "    threshold = 0.05\n",
    "    alphas = [torch.zeros(len(t)) for t in newPrompts]\n",
    "    targetRatios = [torch.ones(len(t)) / len(t) for t in newPrompts]\n",
    "    # 각 prevPrompt에 해당하는 alpha의 업데이트가 필요한지 나타냄.\n",
    "    check = [0] * len(prevPrompts)\n",
    "    for _ in range(30):\n",
    "        for idx in range(len(prevPrompts)):\n",
    "\n",
    "            # alpha를 업데이트하지 않고 모두 0으로 바꾸어 value weight 업데이트가 일어나지 않도록 함.\n",
    "            if check[idx]: \n",
    "                alphas[idx] *= 0\n",
    "                continue\n",
    "        \n",
    "            prevPrompt = prevPrompts[idx]\n",
    "            newPrompt = newPrompts[idx]\n",
    "\n",
    "            # SD model로 prevPrompt에 해당하는 이미지 50개 생성\n",
    "            images = model(prevPrompt, num_images_per_prompt=50, num_inference_steps=20, generator=generator).images\n",
    "            \n",
    "            # score는 생성된 이미지 50개와 텍스트 리스트인 newPrompt 사이의 유사도 점수로 (50, M) 형태. (M은 newPrompt의 길이)\n",
    "            score = clip_model(**clip_processor(text=newPrompt, images=images, return_tensors=\"pt\", padding=True)).logits_per_image\n",
    "\n",
    "            # 각 이미지에 대해 가장 높은 유사도를 가진 점수를 1. 나머지를 0.으로 변환하여 각 이미지에 대해 평균을 냄.\n",
    "            # 즉 ratio는 prevPrompt로 생성된 이미지가 newPrompt의 각 prompt에 해당할 확률을 나타냄.\n",
    "            ratio = score.ge(score.max(1)[0].view(-1,1)).float().mean(0)\n",
    "            # 각 prompt에 해당할 확률이 동일하기를 원하기 때문에 targetRatio와의 차이가 반영 정도가 됨.\n",
    "            alpha = (eta * (targetRatios[idx] - ratio)).to(device)\n",
    "            alphas[idx] = alpha\n",
    "\n",
    "            # ratio와 targetratio와의 차이가 threshold 보다 작다면 더 이상 업데이트가 필요하지 않음.\n",
    "            if ratio.abs().max() < threshold: check[idx] = 1\n",
    "\n",
    "        # 모든 prompt에 대해서 업데이트가 필요하지 않다면 루프를 종료함.\n",
    "        if sum(check) == len(prevPrompts): break\n",
    "\n",
    "        # [(c_i)+sum{alpha/|(W_old)(a)|*|(W_old)(c_i)|*(a)}]\n",
    "        for valueLayer in valueLayers:\n",
    "            reEmbds = []\n",
    "            for idx in range(len(prevPrompts)):\n",
    "        \n",
    "                alpha = alphas[idx]\n",
    "                newPrompt = newPrompts[idx]\n",
    "                prevEmbd = prevEmbds[idx]\n",
    "\n",
    "                newInput = model.tokenizer(newPrompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "                # newEmbd은 (M, 768, 77) 형태의 텍스트 임베딩.\n",
    "                newEmbd = model.text_encoder(newInput)[0].permute(0, 2, 1)\n",
    "                # norm은 (M,) 형태의 벡터로 alpha/|(W_old)(a)|*|(W_old)(c_i)|를 나타냄.\n",
    "                norm = alpha / (valueLayer.weight @ newEmbd).norm(dim=[1,2]) * (valueLayer.weight @ prevEmbd).norm()\n",
    "                \n",
    "                # reEmbd은 (768, 77) 형태의 텍스트 임베딩으로 c_i + sum{alpha/|(W_old)(a)|*|(W_old)(c_i)|*(a)}를 나타냄.\n",
    "                reEmbd = prevEmbd + (norm.view(-1, 1, 1) * newEmbd).sum(0)\n",
    "                reEmbds.append(reEmbd.unsqueeze(0))\n",
    "            reEmbds = torch.concat(reEmbds, 0)\n",
    "\n",
    "            m3 = (reEmbds @ prevEmbds.permute(0, 2, 1)).sum(0) * scale\n",
    "            m3 += lamb * torch.eye(m3.shape[1], device=device)\n",
    "            # m1 = (W_old)[m3]\n",
    "            m1 = valueLayer.weight @ m3\n",
    "            valueLayer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "        \n",
    "    return model\n",
    "\n",
    "# debias\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    concepts = \"Doctor, Nurse, Carpenter\"\n",
    "    attributes = \"male, female\"\n",
    "\n",
    "    concepts = [c.strip() for c in concepts.split(',')]\n",
    "    attributes = [a.strip() for a in attributes.split(',')]\n",
    "\n",
    "    prompts = [\"image of \", \"photo of \", \"portrait of \", \"picture of \", \"\"]\n",
    "    \n",
    "    prevPrompts = []\n",
    "    newPrompts = []\n",
    "    for prompt in prompts:\n",
    "        for concept in concepts:\n",
    "            prevPrompts.append(prompt + concept)\n",
    "            newPrompt = []\n",
    "            for attribute in attributes:\n",
    "                newPrompt.append(prompt + attribute)\n",
    "            newPrompts.append(newPrompt)\n",
    "\n",
    "    lamb = 0.5\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "    model = model.to(device)\n",
    "    model = debiasModel(model=model, prevPrompts=prevPrompts, newPrompts=newPrompts, lamb=lamb,)\n",
    "\n",
    "    torch.save(model.unet.state_dict(), \"model/debias.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a4f9ad3d9f4bf2b555aadaea5cb98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n",
      "100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generateImage(promptPath, modelVersion=\"1.4\", sampleCount=10, imageSize=(512, 512), stepCount=100, guidanceScale=7.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 prompt 파일에 해당하는 이미지를 생성하여 \"image\" 폴더에 저장.\n",
    "\n",
    "    Parameters:\n",
    "        promptPath (str): prompt 파일이 저장된 경로.\n",
    "        modelVersion (str): 이미지 생성시 사용되는 모델의 버전. Default: \"1.4\"\n",
    "        sampleCount (int): 각 prompt마다 생성할 이미지 개수. Default: 10\n",
    "        imageSize (tuple): 생성할 이미지 크기. Default: (512, 512)\n",
    "        stepCount (int): sampling 과정의 inference step 수. Default: 100\n",
    "        guidanceScale (float): classifier-free guidance 강도. Default: 7.5\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if modelVersion == \"1.4\": modelVersion = \"CompVis/stable-diffusion-v1-4\"\n",
    "    elif modelVersion == \"2.1\": modelVersion = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # modelVersion에 따라 모델 선택.\n",
    "    model = StableDiffusionPipeline.from_pretrained(modelVersion).to(device)\n",
    "    vae, tokenizer, textEncoder, unet = model.vae, model.tokenizer, model.text_encoder, model.unet\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "    promptDf = pd.read_csv(promptPath, index_col=0)\n",
    "    # promptDf의 각 행 불러오기.\n",
    "    for _, row in promptDf.iterrows():\n",
    "\n",
    "        # B는 생성할 이미지 개수.\n",
    "        B = sampleCount\n",
    "        H, W = imageSize\n",
    "\n",
    "        # classifier-free guidance를 위해 conditional과 uncoditional prompt가 필요함.\n",
    "        prompts = [row.prompt] * B + [\"\"] * B\n",
    "        inputs = tokenizer(prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        # embds는 (2*B, 768, 77) 형태를 가지는 텍스트 임베딩.\n",
    "        embds = textEncoder(inputs)[0]\n",
    "\n",
    "        seed = row.evaluation_seed\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        # 주어진 시드를 가지는 B개의 latent 생성.\n",
    "        latents = torch.randn((B, unet.in_channels, H//8, W//8), generator=generator).to(device)\n",
    "        latents *= scheduler.init_noise_sigma\n",
    "        \n",
    "        # inference step 수를 설정.\n",
    "        scheduler.set_timesteps(stepCount)\n",
    "        for t in tqdm.tqdm(scheduler.timesteps):\n",
    "\n",
    "            # classifier-free guidance를 위해 uncoditional latent를 추가함.\n",
    "            latentInputs = torch.cat([latents]*2)\n",
    "            # latentInputs를 현재 timestep에 맞게 조정.\n",
    "            latentInputs = scheduler.scale_model_input(latentInputs, timestep=t)\n",
    "\n",
    "            # latent와 텍스트 임베딩을 사용하여 노이즈 생성.\n",
    "            noises = unet(latentInputs, t, encoder_hidden_states=embds).sample\n",
    "            # 앞쪽 B개의 노이즈는 conditional과, 뒤쪽 B개의 노이즈는 uncoditional.\n",
    "            condNoises, uncondNoises = noises.chunk(2)\n",
    "            # classifier-free guidance 적용.\n",
    "            noises = uncondNoises + guidanceScale * (condNoises - uncondNoises)\n",
    "\n",
    "            # 다음 latent 생성.\n",
    "            latents = scheduler.step(noises, t, latents).prev_sample\n",
    "\n",
    "        latents /= 0.18215\n",
    "        # 최종 latent를 decoding하여 이미지 생성.\n",
    "        images = vae.decode(latents).sample\n",
    "\n",
    "        # images의 값을 [0,1]로 제한, 형태를 (B, C, H, W)에서 (B, H, W, C)로 변환.\n",
    "        images = ((images + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        # numpy array 사용을 위해 cpu로 이동.\n",
    "        images = images.detach().cpu().numpy()\n",
    "        # [0,1] 범위의 float를 [0,255] 범위의 uint8로 변환.\n",
    "        images = (images * 255).round().astype(\"uint8\")\n",
    "        # numpy array를 PIL Image로 변환.\n",
    "        images = [Image.fromarray(image) for image in images]\n",
    "        # images에 저장된 B개의 image 저장\n",
    "        for idx, image in enumerate(images):\n",
    "            image.save(f\"image/{row.case_number}_{idx}.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    promptPath = \"data/test_prompts.csv\"\n",
    "    generateImage(promptPath, sampleCount=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

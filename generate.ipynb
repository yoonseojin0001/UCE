{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524ac0bf0f1343149f4bde09a3a4e1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     56\u001b[0m latent_inputs \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_inputs, timestep\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m---> 58\u001b[0m noises \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m     59\u001b[0m cond_noises, uncond_noises \u001b[38;5;241m=\u001b[39m noises\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     60\u001b[0m noises \u001b[38;5;241m=\u001b[39m uncond_noises \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m7.5\u001b[39m \u001b[38;5;241m*\u001b[39m (cond_noises \u001b[38;5;241m-\u001b[39m uncond_noises)\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1141\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sample \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# 1. time\u001b[39;00m\n\u001b[1;32m-> 1141\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_time_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1142\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding(t_emb, timestep_cond)\n\u001b[0;32m   1143\u001b[0m aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:923\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_time_embed\u001b[1;34m(self, sample, timestep)\u001b[0m\n\u001b[0;32m    921\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([timesteps], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(timesteps\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 923\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtimesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[0;32m    926\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mexpand(sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "name = \"default\"\n",
    "\n",
    "with open(f\"data/{name}.yaml\", 'r', encoding='utf-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "version = config[\"version\"]\n",
    "prev_prompt = config[\"prev_prompt\"]\n",
    "retain_prompt = config[\"retain_prompt\"]\n",
    "\n",
    "seed = config[\"seed\"]\n",
    "prompt_count = config[\"prompt_count\"]\n",
    "sample_count = config[\"sample_count\"]\n",
    "\n",
    "prompt = prev_prompt + retain_prompt\n",
    "prompt = prompt[:prompt_count]\n",
    "\n",
    "#\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = UNet2DConditionModel.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"unet\").to(device)\n",
    "unet.load_state_dict(torch.load(f\"model/{name}.pth\"))\n",
    "tokenizer = CLIPTokenizer.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"text_encoder\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"vae\").to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "if not os.path.exists(f\"image/{name}\"):\n",
    "    os.makedirs(f\"image/{name}\")\n",
    "\n",
    "for idx, p in enumerate(tqdm(prompt)):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    token = tokenizer([p] * sample_count + [\"\"] * sample_count, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = text_encoder(token)[0]\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed[idx])\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "    latent *= scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "    for i, img in enumerate(image):\n",
    "        img.save(f\"image/{name}/{idx}_{i}.png\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9a8c4f23e74b2a99b76cabb9476908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An image in the style of Mike Worrall\n",
      "[0.335, 0.315, 0.324, 0.306, 0.326]\n",
      "\n",
      "An image in the style of Jon McCoy\n",
      "[0.238, 0.267, 0.287, 0.226, 0.277]\n",
      "\n",
      "An image in the style of Carne Griffiths\n",
      "[0.354, 0.368, 0.328, 0.366, 0.359]\n",
      "\n",
      "An image in the style of Akos Major\n",
      "[0.32, 0.282, 0.321, 0.323, 0.316]\n",
      "\n",
      "An image in the style of Warren Ellis\n",
      "[0.286, 0.305, 0.317, 0.292, 0.317]\n",
      "\n",
      "An image in the style of Chuck Close\n",
      "[0.326, 0.317, 0.305, 0.314, 0.305]\n",
      "\n",
      "An image in the style of Thomas Struth\n",
      "[0.274, 0.276, 0.281, 0.285, 0.277]\n",
      "\n",
      "An image in the style of Slim Aarons\n",
      "[0.34, 0.306, 0.322, 0.318, 0.342]\n",
      "\n",
      "An image in the style of Aminollah Rezaei\n",
      "[0.279, 0.28, 0.272, 0.289, 0.27]\n",
      "\n",
      "An image in the style of Hans Baldung\n",
      "[0.325, 0.307, 0.313, 0.304, 0.302]\n",
      "\n",
      "An image in the style of Don Maitz\n",
      "[0.294, 0.271, 0.303, 0.292, 0.292]\n",
      "\n",
      "An image in the style of Alessandro Barbucci\n",
      "[0.268, 0.271, 0.259, 0.274, 0.279]\n",
      "\n",
      "An image in the style of Herve Groussin\n",
      "[0.273, 0.271, 0.279, 0.27, 0.306]\n",
      "\n",
      "An image in the style of Steve Argyle\n",
      "[0.226, 0.244, 0.243, 0.232, 0.264]\n",
      "\n",
      "An image in the style of Takato Yamamoto\n",
      "[0.303, 0.269, 0.292, 0.305, 0.311]\n",
      "\n",
      "An image in the style of Hiroshi Yoshida\n",
      "[0.293, 0.311, 0.3, 0.293, 0.278]\n",
      "\n",
      "An image in the style of Wim Wenders\n",
      "[0.271, 0.285, 0.307, 0.283, 0.272]\n",
      "\n",
      "An image in the style of Diego Rivera\n",
      "[0.293, 0.292, 0.299, 0.308, 0.295]\n",
      "\n",
      "An image in the style of Vhils\n",
      "[0.306, 0.314, 0.279, 0.31, 0.303]\n",
      "\n",
      "An image in the style of Alexander Fedosav\n",
      "[0.259, 0.259, 0.269, 0.269, 0.266]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "name = \"default\"\n",
    "\n",
    "with open(f\"data/{name}.yaml\", 'r', encoding='utf-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "version = config[\"version\"]\n",
    "prev_prompt = config[\"prev_prompt\"]\n",
    "retain_prompt = config[\"retain_prompt\"]\n",
    "\n",
    "seed = config[\"seed\"]\n",
    "prompt_count = config[\"prompt_count\"]\n",
    "sample_count = config[\"sample_count\"]\n",
    "\n",
    "prompt = prev_prompt + retain_prompt\n",
    "prompt = prompt[:prompt_count]\n",
    "\n",
    "#\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = UNet2DConditionModel.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"unet\").to(device)\n",
    "unet.load_state_dict(torch.load(f\"model/{name}.pth\"))\n",
    "tokenizer = CLIPTokenizer.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"text_encoder\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"vae\").to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "if not os.path.exists(f\"image/{name}\"):\n",
    "    os.makedirs(f\"image/{name}\")\n",
    "\n",
    "for idx, p in enumerate(tqdm(prompt)):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    token = tokenizer([p] * sample_count + [\"\"] * sample_count, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = text_encoder(token)[0]\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed[idx])\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "    latent *= scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input = processor(text=([p] * sample_count), images=image, return_tensors=\"pt\", padding=True, do_rescale=True)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "\n",
    "    print(p)\n",
    "    print(list(torch.nn.functional.cosine_similarity(image_embd, text_embd).numpy().round(3)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b32cb7d7eea4c03bdd373a87a5d2ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An image in the style of Mike Worrall\n",
      "[0.297, 0.24, 0.215, 0.268, 0.252]\n",
      "\n",
      "An image in the style of Jon McCoy\n",
      "[0.228, 0.198, 0.18, 0.215, 0.248]\n",
      "\n",
      "An image in the style of Carne Griffiths\n",
      "[0.145, 0.204, 0.152, 0.203, 0.16]\n",
      "\n",
      "An image in the style of Akos Major\n",
      "[0.247, 0.16, 0.255, 0.17, 0.25]\n",
      "\n",
      "An image in the style of Warren Ellis\n",
      "[0.245, 0.236, 0.211, 0.22, 0.192]\n",
      "\n",
      "An image in the style of Chuck Close\n",
      "[0.247, 0.174, 0.262, 0.254, 0.269]\n",
      "\n",
      "An image in the style of Thomas Struth\n",
      "[0.235, 0.263, 0.193, 0.252, 0.26]\n",
      "\n",
      "An image in the style of Slim Aarons\n",
      "[0.239, 0.236, 0.22, 0.232, 0.211]\n",
      "\n",
      "An image in the style of Aminollah Rezaei\n",
      "[0.219, 0.257, 0.205, 0.241, 0.239]\n",
      "\n",
      "An image in the style of Hans Baldung\n",
      "[0.244, 0.255, 0.276, 0.295, 0.279]\n",
      "\n",
      "An image in the style of Don Maitz\n",
      "[0.294, 0.265, 0.301, 0.295, 0.287]\n",
      "\n",
      "An image in the style of Alessandro Barbucci\n",
      "[0.265, 0.253, 0.252, 0.272, 0.282]\n",
      "\n",
      "An image in the style of Herve Groussin\n",
      "[0.284, 0.265, 0.283, 0.267, 0.305]\n",
      "\n",
      "An image in the style of Steve Argyle\n",
      "[0.199, 0.248, 0.231, 0.27, 0.268]\n",
      "\n",
      "An image in the style of Takato Yamamoto\n",
      "[0.312, 0.3, 0.313, 0.333, 0.301]\n",
      "\n",
      "An image in the style of Hiroshi Yoshida\n",
      "[0.309, 0.319, 0.291, 0.296, 0.314]\n",
      "\n",
      "An image in the style of Wim Wenders\n",
      "[0.275, 0.29, 0.282, 0.273, 0.274]\n",
      "\n",
      "An image in the style of Diego Rivera\n",
      "[0.3, 0.295, 0.301, 0.314, 0.303]\n",
      "\n",
      "An image in the style of Vhils\n",
      "[0.306, 0.322, 0.258, 0.286, 0.285]\n",
      "\n",
      "An image in the style of Alexander Fedosav\n",
      "[0.263, 0.256, 0.267, 0.244, 0.246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "name = \"default\"\n",
    "\n",
    "with open(f\"data/{name}.yaml\", 'r', encoding='utf-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "version = config[\"version\"]\n",
    "prev_prompt = config[\"prev_prompt\"]\n",
    "retain_prompt = config[\"retain_prompt\"]\n",
    "\n",
    "seed = config[\"seed\"]\n",
    "prompt_count = config[\"prompt_count\"]\n",
    "sample_count = config[\"sample_count\"]\n",
    "\n",
    "prompt = prev_prompt + retain_prompt\n",
    "prompt = prompt[:prompt_count]\n",
    "\n",
    "\n",
    "#\n",
    "name = \"erase1\"\n",
    "\n",
    "#\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = UNet2DConditionModel.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"unet\").to(device)\n",
    "unet.load_state_dict(torch.load(f\"model/{name}.pth\"))\n",
    "tokenizer = CLIPTokenizer.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"text_encoder\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(version, torch_dtype=torch.float16, subfolder=\"vae\").to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "if not os.path.exists(f\"image/{name}\"):\n",
    "    os.makedirs(f\"image/{name}\")\n",
    "\n",
    "for idx, p in enumerate(tqdm(prompt)):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    token = tokenizer([p] * sample_count + [\"\"] * sample_count, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = text_encoder(token)[0]\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed[idx])\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "    latent *= scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input = processor(text=([p] * sample_count), images=image, return_tensors=\"pt\", padding=True, do_rescale=True)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "\n",
    "    print(p)\n",
    "    print(list(torch.nn.functional.cosine_similarity(image_embd, text_embd).numpy().round(3)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831911aed6b74aba88282bea7c00247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An image in the style of Mike Worrall\n",
      "[0.232, 0.205, 0.199, 0.259, 0.224]\n",
      "\n",
      "An image in the style of Jon McCoy\n",
      "[0.218, 0.2, 0.215, 0.156, 0.205]\n",
      "\n",
      "An image in the style of Carne Griffiths\n",
      "[0.233, 0.187, 0.163, 0.24, 0.186]\n",
      "\n",
      "An image in the style of Akos Major\n",
      "[0.172, 0.145, 0.207, 0.162, 0.174]\n",
      "\n",
      "An image in the style of Warren Ellis\n",
      "[0.229, 0.211, 0.222, 0.21, 0.187]\n",
      "\n",
      "An image in the style of Chuck Close\n",
      "[0.202, 0.203, 0.221, 0.18, 0.182]\n",
      "\n",
      "An image in the style of Thomas Struth\n",
      "[0.245, 0.249, 0.215, 0.226, 0.246]\n",
      "\n",
      "An image in the style of Slim Aarons\n",
      "[0.226, 0.207, 0.221, 0.178, 0.181]\n",
      "\n",
      "An image in the style of Aminollah Rezaei\n",
      "[0.242, 0.23, 0.215, 0.236, 0.255]\n",
      "\n",
      "An image in the style of Hans Baldung\n",
      "[0.215, 0.203, 0.233, 0.238, 0.201]\n",
      "\n",
      "An image in the style of Don Maitz\n",
      "[0.166, 0.226, 0.227, 0.231, 0.195]\n",
      "\n",
      "An image in the style of Alessandro Barbucci\n",
      "[0.228, 0.234, 0.245, 0.243, 0.24]\n",
      "\n",
      "An image in the style of Herve Groussin\n",
      "[0.225, 0.262, 0.238, 0.243, 0.245]\n",
      "\n",
      "An image in the style of Steve Argyle\n",
      "[0.222, 0.209, 0.203, 0.222, 0.217]\n",
      "\n",
      "An image in the style of Takato Yamamoto\n",
      "[0.219, 0.143, 0.169, 0.127, 0.117]\n",
      "\n",
      "An image in the style of Hiroshi Yoshida\n",
      "[0.18, 0.239, 0.197, 0.201, 0.228]\n",
      "\n",
      "An image in the style of Wim Wenders\n",
      "[0.257, 0.185, 0.211, 0.193, 0.223]\n",
      "\n",
      "An image in the style of Diego Rivera\n",
      "[0.221, 0.184, 0.188, 0.23, 0.191]\n",
      "\n",
      "An image in the style of Vhils\n",
      "[0.194, 0.204, 0.191, 0.152, 0.207]\n",
      "\n",
      "An image in the style of Alexander Fedosav\n",
      "[0.213, 0.251, 0.258, 0.255, 0.213]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "name = \"erase3\"\n",
    "\n",
    "#\n",
    "unet.load_state_dict(torch.load(f\"model/{name}.pth\"))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "if not os.path.exists(f\"image/{name}\"):\n",
    "    os.makedirs(f\"image/{name}\")\n",
    "\n",
    "for idx, p in enumerate(tqdm(prompt)):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    token = tokenizer([p] * sample_count + [\"\"] * sample_count, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = text_encoder(token)[0]\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed[idx])\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "    latent *= scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input = processor(text=([p] * sample_count), images=image, return_tensors=\"pt\", padding=True, do_rescale=True)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "\n",
    "    print(p)\n",
    "    print(list(torch.nn.functional.cosine_similarity(image_embd, text_embd).numpy().round(3)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([[0.232, 0.205, 0.199, 0.259, 0.224],\n",
    "[0.218, 0.2, 0.215, 0.156, 0.205],\n",
    "[0.233, 0.187, 0.163, 0.24, 0.186],\n",
    "[0.172, 0.145, 0.207, 0.162, 0.174],\n",
    "[0.229, 0.211, 0.222, 0.21, 0.187],\n",
    "[0.202, 0.203, 0.221, 0.18, 0.182],\n",
    "[0.245, 0.249, 0.215, 0.226, 0.246],\n",
    "[0.226, 0.207, 0.221, 0.178, 0.181],\n",
    "[0.242, 0.23, 0.215, 0.236, 0.255],\n",
    "[0.215, 0.203, 0.233, 0.238, 0.201]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([[0.232, 0.205, 0.199, 0.259, 0.224],\n",
    "[0.218, 0.2, 0.215, 0.156, 0.205],\n",
    "[0.233, 0.187, 0.163, 0.24, 0.186],\n",
    "[0.172, 0.145, 0.207, 0.162, 0.174],\n",
    "[0.229, 0.211, 0.222, 0.21, 0.187],\n",
    "[0.202, 0.203, 0.221, 0.18, 0.182],\n",
    "[0.245, 0.249, 0.215, 0.226, 0.246],\n",
    "[0.226, 0.207, 0.221, 0.178, 0.181],\n",
    "[0.242, 0.23, 0.215, 0.236, 0.255],\n",
    "[0.215, 0.203, 0.233, 0.238, 0.201]])\n",
    "\n",
    "[[0.166, 0.226, 0.227, 0.231, 0.195],\n",
    "[0.228, 0.234, 0.245, 0.243, 0.24],\n",
    "[0.225, 0.262, 0.238, 0.243, 0.245],\n",
    "[0.222, 0.209, 0.203, 0.222, 0.217],\n",
    "[0.219, 0.143, 0.169, 0.127, 0.117],\n",
    "[0.18, 0.239, 0.197, 0.201, 0.228],\n",
    "[0.257, 0.185, 0.211, 0.193, 0.223],\n",
    "[0.221, 0.184, 0.188, 0.23, 0.191],\n",
    "[0.194, 0.204, 0.191, 0.152, 0.207],\n",
    "[0.213, 0.251, 0.258, 0.255, 0.213]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([[0.166, 0.226, 0.227, 0.231, 0.195],\n",
    "[0.228, 0.234, 0.245, 0.243, 0.24],\n",
    "[0.225, 0.262, 0.238, 0.243, 0.245],\n",
    "[0.222, 0.209, 0.203, 0.222, 0.217],\n",
    "[0.219, 0.143, 0.169, 0.127, 0.117],\n",
    "[0.18, 0.239, 0.197, 0.201, 0.228],\n",
    "[0.257, 0.185, 0.211, 0.193, 0.223],\n",
    "[0.221, 0.184, 0.188, 0.23, 0.191],\n",
    "[0.194, 0.204, 0.191, 0.152, 0.207],\n",
    "[0.213, 0.251, 0.258, 0.255, 0.213]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ef26e507d74dadbe78d24d81818fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An image in the style of Mike Worrall\n",
      "[0.305, 0.311, 0.338, 0.297, 0.334]\n",
      "\n",
      "An image in the style of Jon McCoy\n",
      "[0.252, 0.291, 0.238, 0.223, 0.271]\n",
      "\n",
      "An image in the style of Carne Griffiths\n",
      "[0.341, 0.367, 0.32, 0.367, 0.368]\n",
      "\n",
      "An image in the style of Akos Major\n",
      "[0.302, 0.281, 0.304, 0.255, 0.302]\n",
      "\n",
      "An image in the style of Warren Ellis\n",
      "[0.291, 0.272, 0.317, 0.307, 0.306]\n",
      "\n",
      "An image in the style of Chuck Close\n",
      "[0.283, 0.236, 0.27, 0.275, 0.272]\n",
      "\n",
      "An image in the style of Thomas Struth\n",
      "[0.287, 0.273, 0.28, 0.295, 0.281]\n",
      "\n",
      "An image in the style of Slim Aarons\n",
      "[0.339, 0.314, 0.281, 0.278, 0.321]\n",
      "\n",
      "An image in the style of Aminollah Rezaei\n",
      "[0.251, 0.276, 0.294, 0.278, 0.266]\n",
      "\n",
      "An image in the style of Hans Baldung\n",
      "[0.273, 0.291, 0.305, 0.283, 0.306]\n",
      "\n",
      "An image in the style of Don Maitz\n",
      "[0.296, 0.281, 0.292, 0.292, 0.291]\n",
      "\n",
      "An image in the style of Alessandro Barbucci\n",
      "[0.256, 0.247, 0.252, 0.241, 0.231]\n",
      "\n",
      "An image in the style of Herve Groussin\n",
      "[0.219, 0.24, 0.266, 0.237, 0.256]\n",
      "\n",
      "An image in the style of Steve Argyle\n",
      "[0.248, 0.245, 0.247, 0.252, 0.254]\n",
      "\n",
      "An image in the style of Takato Yamamoto\n",
      "[0.139, 0.097, 0.162, 0.135, 0.099]\n",
      "\n",
      "An image in the style of Hiroshi Yoshida\n",
      "[0.282, 0.294, 0.293, 0.274, 0.299]\n",
      "\n",
      "An image in the style of Wim Wenders\n",
      "[0.27, 0.284, 0.265, 0.291, 0.256]\n",
      "\n",
      "An image in the style of Diego Rivera\n",
      "[0.299, 0.318, 0.305, 0.301, 0.317]\n",
      "\n",
      "An image in the style of Vhils\n",
      "[0.307, 0.287, 0.316, 0.292, 0.307]\n",
      "\n",
      "An image in the style of Alexander Fedosav\n",
      "[0.262, 0.274, 0.255, 0.257, 0.264]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "name = \"erase4\"\n",
    "\n",
    "#\n",
    "unet.load_state_dict(torch.load(f\"model/{name}.pth\"))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "if not os.path.exists(f\"image/{name}\"):\n",
    "    os.makedirs(f\"image/{name}\")\n",
    "\n",
    "for idx, p in enumerate(tqdm(prompt)):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    token = tokenizer([p] * sample_count + [\"\"] * sample_count, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = text_encoder(token)[0]\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed[idx])\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "    latent *= scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input = processor(text=([p] * sample_count), images=image, return_tensors=\"pt\", padding=True, do_rescale=True)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "\n",
    "    print(p)\n",
    "    print(list(torch.nn.functional.cosine_similarity(image_embd, text_embd).numpy().round(3)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "0.25-0.28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "artist_idx = 6\n",
    "with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)\n",
    "artist = artist[artist_idx]\n",
    "\n",
    "#\n",
    "sample_count = 10\n",
    "prompt = [f\"An image in the style of {artist}\"]\n",
    "\n",
    "image = []\n",
    "for _ in range(sample_count):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"]\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((1, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image.append(pipeline.vae.decode(latent).sample.detach().cpu()[0])\n",
    "\n",
    "image = np.stack(image)\n",
    "image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "image = (image * 255).round().astype(\"uint8\")\n",
    "image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "#\n",
    "input = processor(text=prompt * sample_count, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "output = model(**input)\n",
    "image_embd = output.image_embeds\n",
    "text_embd = output.text_embeds\n",
    "\n",
    "score = torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy()\n",
    "for s in score:\n",
    "    print(f\"{s:.3f}\", end=\" \")\n",
    "print(f\"\\tMean: {score.mean():.3f}\\tStd: {score.std(ddof=1):.3f}\\t{artist}\")\n",
    "\n",
    "for i in image:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet2DConditionModel\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPTextModel, CLIPTokenizer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\import_utils.py:821\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    820\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 821\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\import_utils.py:820\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    818\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 820\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\import_utils.py:830\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 830\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    833\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    834\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    835\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet_1d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet1DModel\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet2DModel\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet_2d_condition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet2DConditionModel\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet_3d_condition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet3DConditionModel\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianFourierProjection, TimestepEmbedding, Timesteps\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMixin\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet_2d_blocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNetMidBlock2D, get_down_block, get_up_block\n\u001b[0;32m     27\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUNet2DOutput\u001b[39;00m(BaseOutput):\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    The output of [`UNet2DModel`].\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m            The hidden states output from the last layer of the model.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:36\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaGroupNorm\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     Downsample2D,\n\u001b[0;32m     28\u001b[0m     FirDownsample2D,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     Upsample2D,\n\u001b[0;32m     35\u001b[0m )\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdual_transformer_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DualTransformer2DModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer2DModel\n\u001b[0;32m     40\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\transformers\\__init__.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlumina_nextdit2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LuminaNextDiT2DModel\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixart_transformer_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PixArtTransformer2DModel\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprior_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PriorTransformer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstable_audio_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableAudioDiTModel\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_film_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5FilmDecoder\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\transformers\\prior_transformer.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigMixin, register_to_config\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, UNet2DConditionLoadersMixin\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOutput\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicTransformerBlock\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\import_utils.py:820\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    818\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 820\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\import_utils.py:830\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 830\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    833\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    834\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    835\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\loaders\\unet.py:46\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model_dict_into_meta, load_state_dict\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     USE_PEFT_BACKEND,\n\u001b[0;32m     37\u001b[0m     _get_model_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     logging,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LORA_WEIGHT_NAME, LORA_WEIGHT_NAME_SAFE, TEXT_ENCODER_NAME, UNET_NAME\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttnProcsLayers\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\loaders\\lora_pipeline.py:33\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_hf_hub_args\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     USE_PEFT_BACKEND,\n\u001b[0;32m     22\u001b[0m     convert_state_dict_to_diffusers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     scale_lora_layers,\n\u001b[0;32m     32\u001b[0m )\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraBaseMixin\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora_conversion_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _convert_non_diffusers_lora_to_diffusers, _maybe_map_sgm_blocks_to_diffusers\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_transformers_available():\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\loaders\\lora_base.py:44\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     USE_PEFT_BACKEND,\n\u001b[0;32m     30\u001b[0m     _get_model_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     set_weights_and_activate_adapters,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_transformers_available():\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_available():\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTunerLayer\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:59\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_module_from_name\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafetensors_conversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_conversion\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, AutoQuantizationConfig\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfQuantizer\n",
      "File \u001b[1;32mc:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\auto.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer_bitnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BitNetHfQuantizer\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer_bnb_4bit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bnb4BitHfQuantizer\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer_bnb_8bit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bnb8BitHfQuantizer\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer_compressed_tensors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompressedTensorsHfQuantizer\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer_eetq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EetqHfQuantizer\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = UNet2DConditionModel.from_pretrained(version, subfolder=\"unet\").to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(version, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(version, subfolder=\"text_encoder\").to(device)\n",
    "\n",
    "#\n",
    "with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    valid_artist = yaml.safe_load(file)\n",
    "with open(f\"data/invalid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    invalid_artist = yaml.safe_load(file)\n",
    "\n",
    "# erase_artist = [\"Ernie Barnes\", \"Alain Laboile\", \"Ron Mueck\", \"Mickalene Thomas\"]\n",
    "# retain_artist = list(set(valid_artist) - set(erase_artist)) + invalid_artist\n",
    "# erase_artist = [\"Ernie Barnes\", \"Alain Laboile\"]\n",
    "\n",
    "# erase_artist = [\"Ernie Barnes\"]\n",
    "# retain_artist = list(set(valid_artist) - set(erase_artist)) + invalid_artist\n",
    "\n",
    "erase_artist = valid_artist\n",
    "retain_artist = []\n",
    "\n",
    "\n",
    "prev_prompt = [f\"An image in the style of {a}\" for a in erase_artist]\n",
    "new_prompt = [\"Art\"] * len(prev_prompt)\n",
    "retain_prompt = [f\"An image in the style of {a}\" for a in retain_artist]\n",
    "\n",
    "#\n",
    "lamb = 0.5\n",
    "erase_scale = 0.5\n",
    "preserve_scale = 0.1\n",
    "with_key = True\n",
    "\n",
    "ca_layer = []\n",
    "for n, module in unet.named_modules():\n",
    "    if n[-5:] != \"attn2\": continue\n",
    "    ca_layer.append(module)\n",
    "\n",
    "value_layer = [layer.to_v for layer in ca_layer]\n",
    "target_layer = value_layer\n",
    "\n",
    "if with_key:\n",
    "    key_layer = [layer.to_k for layer in ca_layer]\n",
    "    target_layer += key_layer\n",
    "\n",
    "m2 = lamb * torch.eye(1024, device=device)\n",
    "m3 = lamb * torch.eye(1024, device=device)\n",
    "\n",
    "count = (len(prev_prompt) - 1) // 300 + 1\n",
    "for c in range(count):\n",
    "    prev_token = tokenizer(prev_prompt[300 * c:300 * (c + 1)], padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prev_embd = text_encoder(prev_token)[0].permute(0, 2, 1)\n",
    "\n",
    "    new_token = tokenizer(new_prompt[300 * c:300 * (c + 1)], padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    new_embd = text_encoder(new_token)[0].permute(0, 2, 1)\n",
    "\n",
    "    m2 += (prev_embd @ prev_embd.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m3 += (new_embd @ prev_embd.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "\n",
    "if retain_prompt:\n",
    "    count = (len(retain_prompt) - 1) // 300 + 1\n",
    "    for c in range(count):\n",
    "        retain_token = tokenizer(retain_prompt[300 * c:300 * (c + 1)], padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        retain_embd = text_encoder(retain_token)[0].permute(0, 2, 1)\n",
    "\n",
    "        m2 += (retain_embd @ retain_embd.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "        m3 += (retain_embd @ retain_embd.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "\n",
    "for layer in target_layer:\n",
    "    m1 = layer.weight @ m3\n",
    "    layer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "\n",
    "#\n",
    "print(f\"{lamb}\\t{erase_scale}\\t{preserve_scale}\\t{with_key}\")\n",
    "\n",
    "torch.save(unet.state_dict(), f\"model/erase.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "pipeline.unet.load_state_dict(torch.load(f\"model/erase.pth\"))\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "# artist_idx = 0\n",
    "# with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "#     artist = yaml.safe_load(file)\n",
    "# artist = artist[artist_idx]\n",
    "artist = \"Ernie Barnes\"\n",
    "\n",
    "#\n",
    "sample_count = 10\n",
    "prompt = [f\"An image in the style of {artist}\"]\n",
    "\n",
    "image = []\n",
    "for _ in range(sample_count):\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"]\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((1, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image.append(pipeline.vae.decode(latent).sample.detach().cpu()[0])\n",
    "\n",
    "image = np.stack(image)\n",
    "image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "image = (image * 255).round().astype(\"uint8\")\n",
    "image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "#\n",
    "input = processor(text=prompt * sample_count, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "output = model(**input)\n",
    "image_embd = output.image_embeds\n",
    "text_embd = output.text_embeds\n",
    "\n",
    "score = torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy()\n",
    "for s in score:\n",
    "    print(f\"{s:.3f}\", end=\" \")\n",
    "print(f\"\\tMean: {score.mean():.3f}\\tStd: {score.std(ddof=1):.3f}\")\n",
    "\n",
    "for i in image:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be60fe5f48549708be2ab287c8c361f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
    "\n",
    "#\n",
    "start_time = time.time()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "pipeline.scheduler = PNDMScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
    "\n",
    "#\n",
    "start_time = time.time()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "artist = \"Frank Tinsley\"\n",
    "prompt = f\"An image in the style of {artist}\"\n",
    "\n",
    "image_count = 5\n",
    "images = pipeline(prompt, num_inference_steps=50, num_images_per_prompt=image_count).images\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "end_time = time.time()\n",
    "print(f\"#\\nScheduler: {pipeline.scheduler._class_name}\\nTime: {end_time - start_time:.1f}\")\n",
    "print(f\"Artist: {artist}\\nScore:\")\n",
    "\n",
    "#\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained=\"laion2b_s32b_b79k\")\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "\n",
    "image_embds = model.encode_image(torch.cat([preprocess(image).unsqueeze(0) for image in images], dim=0))\n",
    "image_embds /= image_embds.norm(dim=1, keepdim=True)\n",
    "\n",
    "text_embd = model.encode_text(tokenizer(prompt))[0]\n",
    "text_embd /= text_embd.norm()\n",
    "\n",
    "scores = []\n",
    "for image_embd in image_embds:\n",
    "    scores.append(torch.sum(image_embd * text_embd).item())\n",
    "scores = torch.tensor(scores)\n",
    "for score in scores:\n",
    "    print(f\"{score:.3f}\", end=\" \")\n",
    "print(f\"\\tMean: {scores.mean():.3f}\\tViT-H-14\")\n",
    "\n",
    "#\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "input = processor(text=[prompt] * image_count, images=images, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "output = model(**input)\n",
    "image_embds = output.image_embeds\n",
    "text_embds = output.text_embeds\n",
    "\n",
    "scores = torch.nn.functional.cosine_similarity(image_embds, text_embds).detach().cpu().numpy()\n",
    "for score in scores:\n",
    "    print(f\"{score:.3f}\", end=\" \")\n",
    "print(f\"\\tMean: {scores.mean():.3f}\")\n",
    "\n",
    "for image in images:\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "artist = \"Frank Tinsley\"\n",
    "prompt = f\"An image in the style of {artist}\"\n",
    "\n",
    "image_count = 5\n",
    "images = pipeline(prompt, num_inference_steps=50, num_images_per_prompt=image_count).images\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Artist: {artist}\\nScore:\")  \n",
    "\n",
    "#\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained=\"laion2b_s32b_b79k\")\n",
    "model.eval()\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "\n",
    "image_embds = model.encode_image(torch.cat([preprocess(image).unsqueeze(0) for image in images], dim=0))\n",
    "image_embds /= image_embds.norm(dim=1, keepdim=True)\n",
    "\n",
    "text_embd = model.encode_text(tokenizer(prompt))[0]\n",
    "text_embd /= text_embd.norm()\n",
    "\n",
    "scores = []\n",
    "for image_embd in image_embds:\n",
    "    scores.append(torch.sum(image_embd * text_embd).item())\n",
    "scores = torch.tensor(scores)\n",
    "for score in scores:\n",
    "    print(f\"{score:.3f}\", end=\" \")\n",
    "print(f\"\\tMean: {scores.mean():.3f}\")\n",
    "\n",
    "for image in images:\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "#\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = UNet2DConditionModel.from_pretrained(version, subfolder=\"unet\").to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(version, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(version, subfolder=\"text_encoder\").to(device)\n",
    "\n",
    "#\n",
    "with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)\n",
    "random.shuffle(artist)\n",
    "with open(f\"data/valid_artist.yaml\", 'w') as file:\n",
    "    yaml.dump(artist, file)\n",
    "\n",
    "prompt = [\"An image in the style of \" + a for a in artist]\n",
    "prev_prompt = prompt[:10]\n",
    "new_prompt = [\"art\"] * 10\n",
    "retain_prompt = prompt[10:]\n",
    "\n",
    "#\n",
    "lamb = 0.5\n",
    "erase_scale = 1\n",
    "preserve_scale = 0.1\n",
    "with_key = True\n",
    "\n",
    "ca_layer = []\n",
    "for n, module in unet.named_modules():\n",
    "    if n[-5:] != \"attn2\": continue\n",
    "    ca_layer.append(module)\n",
    "\n",
    "value_layer = [layer.to_v for layer in ca_layer]\n",
    "target_layer = value_layer\n",
    "\n",
    "if with_key:\n",
    "    key_layer = [layer.to_k for layer in ca_layer]\n",
    "    target_layer += key_layer\n",
    "\n",
    "prev_token = tokenizer(prev_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "prev_embd = text_encoder(prev_token)[0].permute(0, 2, 1)\n",
    "\n",
    "new_token = tokenizer(new_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "new_embd = text_encoder(new_token)[0].permute(0, 2, 1)\n",
    "\n",
    "m2 = (prev_embd @ prev_embd.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "m3 = (new_embd @ prev_embd.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "if retain_prompt:\n",
    "    retain_token = tokenizer(retain_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    retain_embd = text_encoder(retain_token)[0].permute(0, 2, 1)\n",
    "\n",
    "    m2 += (retain_embd @ retain_embd.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "    m3 += (retain_embd @ retain_embd.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "\n",
    "for layer in target_layer:\n",
    "    m1 = layer.weight @ m3\n",
    "    layer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "\n",
    "torch.save(unet.state_dict(), f\"model/erase10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5865eac5286f40b4be7ee337417cd4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2349, 0.2348, 0.247, 0.237, 0.2179], [0.266, 0.2795, 0.2411, 0.2601, 0.273], [0.2534, 0.2572, 0.2589, 0.2699, 0.2691], [0.2588, 0.2418, 0.2476, 0.2524, 0.2294], [0.2718, 0.2488, 0.2624, 0.2488, 0.2824], [0.2563, 0.2429, 0.2458, 0.2505, 0.2509], [0.2865, 0.2657, 0.2635, 0.2755, 0.2707], [0.2506, 0.2577, 0.2543, 0.2678, 0.2602], [0.2612, 0.2894, 0.2697, 0.2879, 0.2559], [0.246, 0.27, 0.2508, 0.2438, 0.2633]]\n",
      "0.2576    0.0151\n",
      "[[0.2494, 0.2456, 0.2497, 0.2534, 0.2565], [0.2511, 0.2582, 0.2654, 0.279, 0.257], [0.26, 0.2479, 0.2624, 0.2558, 0.2692], [0.2256, 0.2252, 0.24, 0.2385, 0.2555], [0.2568, 0.2738, 0.2551, 0.2685, 0.2812], [0.2439, 0.2356, 0.2206, 0.2399, 0.2248], [0.2558, 0.2603, 0.2677, 0.2651, 0.2685], [0.2504, 0.2544, 0.2434, 0.2488, 0.2471], [0.2933, 0.2675, 0.2706, 0.274, 0.288], [0.2462, 0.2379, 0.2374, 0.2586, 0.2567]]\n",
      "0.2547    0.0156\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "artist = [\"Liora Jansen\", \"Esmond Vale\", \"Alira Keats\", \"Soren Elwood\", \"Mirae Lorne\", \n",
    "          \"Tavish Cromwell\", \"Elara Fawn\", \"Casper Thorne\", \"Vera Lysander\", \"Kaelan Rivers\"]\n",
    "artist = [\"Evelyn Hartley\", \"Liam Whitaker\", \"Ava Sterling\", \"Mason Caldwell\", \"Isla Kensington\", \n",
    "         \"Ethan Marlowe\", \"Zoe Whitman\", \"Leo Farnsworth\", \"Clara Beaumont\", \"Asher Langford\"]\n",
    "\n",
    "#\n",
    "sample_count = 5\n",
    "\n",
    "default_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    default_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "pipeline.unet.load_state_dict(torch.load(f\"model/erase10.pth\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "erased_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    erased_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "print(default_score)\n",
    "print(f\"{np.mean(default_score[:10]):.4f}    {np.std(default_score[:10]):.4f}\")\n",
    "print(erased_score)\n",
    "print(f\"{np.mean(erased_score[:10]):.4f}    {np.std(erased_score[:10]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "artist = [\"Augustus Whitaker\", \"Eleanor Pembroke\", \"Theodore Langford\", \"Beatrice Sinclair\", \"Sebastian Hawthorne\", \n",
    "         \"Vivienne Montclair\", \"Leonard Fairfax\", \"Cecilia Harrington\", \"Edmund Ashford\", \"Isabella Worthington\"]\n",
    "# artist = [\"Alaric Thorne\", \"Beatrice Elmsworth\", \"Cedric Wycliffe\", \"Eleanor Ravenscroft\", \"Fitzgerald Montague\", \n",
    "#          \"Gwendolyn Fairchild\", \"Horatio Pembroke\", \"Isolde Ashbourne\", \"Percival Langley\", \"Seraphina Blackwood\"]\n",
    "\n",
    "#\n",
    "sample_count = 5\n",
    "\n",
    "default_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    default_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "pipeline.unet.load_state_dict(torch.load(f\"model/erase10.pth\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "erased_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    erased_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "print(default_score)\n",
    "print(f\"{np.mean(default_score[:10]):.4f}    {np.std(default_score[:10]):.4f}\")\n",
    "print(erased_score)\n",
    "print(f\"{np.mean(erased_score[:10]):.4f}    {np.std(erased_score[:10]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540a04408773490e9b5385b86a9e781e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2403, 0.238, 0.2142, 0.2505, 0.2324], [0.3182, 0.2889, 0.3309, 0.3062, 0.3041], [0.1673, 0.1492, 0.1391, 0.1472, 0.1778], [0.1401, 0.1798, 0.1459, 0.1411, 0.1479], [0.1613, 0.1328, 0.1312, 0.1263, 0.1515], [0.2122, 0.1966, 0.1824, 0.2104, 0.2067], [0.1966, 0.206, 0.2071, 0.2485, 0.2036], [0.289, 0.2721, 0.2724, 0.2984, 0.3256], [0.2325, 0.2176, 0.1817, 0.2285, 0.2037], [0.2448, 0.2584, 0.2646, 0.2385, 0.2291], [0.0715, 0.0741, 0.1111, 0.0483, 0.1058], [0.2927, 0.3145, 0.2716, 0.3167, 0.2559], [0.2357, 0.2326, 0.1875, 0.1858, 0.1907], [0.1627, 0.1814, 0.1924, 0.1861, 0.1666], [0.2328, 0.2259, 0.2333, 0.2299, 0.2177], [0.1663, 0.2252, 0.179, 0.1751, 0.1492], [0.1559, 0.1367, 0.1274, 0.1526, 0.1266], [0.1439, 0.1972, 0.1601, 0.1799, 0.1951], [0.23, 0.2646, 0.2088, 0.2896, 0.2438], [0.1884, 0.2014, 0.1664, 0.1627, 0.2193]]\n",
      "0.2158    0.0564\n",
      "0.1914    0.0582\n",
      "[[0.2462, 0.2604, 0.2428, 0.2222, 0.2172], [0.1997, 0.2053, 0.2404, 0.1879, 0.2027], [0.1363, 0.1571, 0.1485, 0.1444, 0.1627], [0.1369, 0.1309, 0.1456, 0.1367, 0.1354], [0.0817, 0.0985, 0.1188, 0.1327, 0.1324], [0.2457, 0.1864, 0.2388, 0.2099, 0.1762], [0.2267, 0.2031, 0.2032, 0.1808, 0.1838], [0.1712, 0.1553, 0.2829, 0.2017, 0.2185], [0.2114, 0.2089, 0.2518, 0.2145, 0.202], [0.2365, 0.2249, 0.2377, 0.1899, 0.1977], [0.0732, 0.0898, 0.0601, 0.065, 0.0897], [0.3319, 0.307, 0.2106, 0.2984, 0.2791], [0.2646, 0.2527, 0.2185, 0.2699, 0.2588], [0.1743, 0.195, 0.1446, 0.1584, 0.1567], [0.228, 0.2279, 0.2294, 0.227, 0.2475], [0.1747, 0.1455, 0.2009, 0.1466, 0.129], [0.1515, 0.148, 0.1641, 0.1223, 0.1434], [0.2337, 0.2128, 0.2182, 0.2019, 0.1809], [0.1851, 0.2438, 0.2296, 0.2582, 0.1829], [0.1866, 0.1993, 0.174, 0.1638, 0.1751]]\n",
      "0.1897    0.0448\n",
      "0.1926    0.0612\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#\n",
    "version = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(version, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "with open(f\"data/valid_artist.yaml\", 'r', encoding='utf-8') as file:\n",
    "    artist = yaml.safe_load(file)[:20]\n",
    "\n",
    "#\n",
    "sample_count = 5\n",
    "\n",
    "default_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    for t in token: t[7: list(t.numpy()).index(49407) - 1] = 0\n",
    "    token = token.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    default_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "pipeline.unet.load_state_dict(torch.load(f\"model/erase10.pth\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "erased_score = []\n",
    "for a in artist:\n",
    "    prompt = [f\"An image in the style of {a}\"] * sample_count\n",
    "\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(100)\n",
    "\n",
    "    cfg_prompt = prompt + [\"\"] * sample_count\n",
    "    token = pipeline.tokenizer(cfg_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    for t in token: t[7: list(t.numpy()).index(49407) - 1] = 0\n",
    "    token = token.to(device)\n",
    "    embd = pipeline.text_encoder(token)[0]\n",
    "\n",
    "    latent = torch.randn((sample_count, 4, 64, 64), device=device, dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latent] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "        noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "        cond_noise, uncond_noise = noise.chunk(2)\n",
    "        noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "        latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "    latent /= 0.18215\n",
    "    image = pipeline.vae.decode(latent).sample.detach().cpu().numpy()\n",
    "    image = ((image + 1) / 2).clip(0, 1).transpose(0, 2, 3, 1)\n",
    "    image = (image * 255).round().astype(\"uint8\")\n",
    "    image = [Image.fromarray(i) for i in image]\n",
    "\n",
    "    input = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, do_rescale=True).to(device)\n",
    "    output = model(**input)\n",
    "    image_embd = output.image_embeds\n",
    "    text_embd = output.text_embeds\n",
    "    score = list(torch.nn.functional.cosine_similarity(image_embd, text_embd).detach().cpu().numpy().round(4))\n",
    "    erased_score.append(score)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#\n",
    "print(default_score)\n",
    "print(f\"{np.mean(default_score[:10]):.4f}    {np.std(default_score[:10]):.4f}\")\n",
    "print(f\"{np.mean(default_score[10:]):.4f}    {np.std(default_score[10:]):.4f}\")\n",
    "print(erased_score)\n",
    "print(f\"{np.mean(erased_score[:10]):.4f}    {np.std(erased_score[:10]):.4f}\")\n",
    "print(f\"{np.mean(erased_score[10:]):.4f}    {np.std(erased_score[10:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

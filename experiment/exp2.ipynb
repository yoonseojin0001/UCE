{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lpips\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/artists1734_prompts.csv\")\n",
    "\n",
    "artists = list(df.artist.unique())\n",
    "random.shuffle(artists)\n",
    "\n",
    "prompts = artists[:10]\n",
    "seeds = [random.randint(0, 5000) for _ in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline):\n",
    "    del pipeline.vae\n",
    "    del pipeline.tokenizer\n",
    "    del pipeline.text_encoder\n",
    "    del pipeline.unet\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_autohalf_images(pipeline, prompts, seeds):\n",
    "    device = pipeline.device\n",
    "    images = []\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "            scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "            scheduler.set_timesteps(100)\n",
    "\n",
    "            prompt = [prompt] + [\"\"]\n",
    "            \n",
    "            token = pipeline.tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids.to(device)\n",
    "            \n",
    "            embd = pipeline.text_encoder(token)[0]\n",
    "            \n",
    "            seed = seeds[idx]\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            \n",
    "            latent = torch.randn((1, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "            latent *= scheduler.init_noise_sigma\n",
    "            \n",
    "            for t in scheduler.timesteps:\n",
    "                latent_input = torch.cat([latent] * 2)\n",
    "                latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "                \n",
    "                noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "                cond_noise, uncond_noise = noise.chunk(2)\n",
    "                noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "                \n",
    "                latent = scheduler.step(noise, t, latent).prev_sample\n",
    "            \n",
    "            image = pipeline.vae.decode(latent).sample\n",
    "            image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "            image = image.detach().cpu().numpy()\n",
    "            image = (image * 255).round().astype(\"uint8\")\n",
    "            image = Image.fromarray(image[0])\n",
    "            images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_half_images(pipeline, prompts, seeds):\n",
    "    device = pipeline.device\n",
    "    images = []\n",
    "    \n",
    "    for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "        scheduler.set_timesteps(100)\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "        \n",
    "        token = pipeline.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "        \n",
    "        embd = pipeline.text_encoder(token)[0]\n",
    "        \n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        latent = torch.randn((1, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "        \n",
    "        for t in scheduler.timesteps:\n",
    "            latent_input = torch.cat([latent] * 2)\n",
    "            latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "            \n",
    "            noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "            cond_noise, uncond_noise = noise.chunk(2)\n",
    "            noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "            \n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "        \n",
    "        image = pipeline.vae.decode(latent).sample\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image[0])\n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_images(pipeline, prompts, seeds):\n",
    "    device = pipeline.device\n",
    "    images = []\n",
    "    \n",
    "    for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "        scheduler.set_timesteps(100)\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "        \n",
    "        token = pipeline.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "        embd = pipeline.text_encoder(token)[0]\n",
    "        \n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        latent = torch.randn((1, 4, 64, 64), generator=generator, device=device)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "        \n",
    "        for t in scheduler.timesteps:\n",
    "            latent_input = torch.cat([latent] * 2)\n",
    "            latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "            \n",
    "            noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "            cond_noise, uncond_noise = noise.chunk(2)\n",
    "            noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "            \n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "        \n",
    "        image = pipeline.vae.decode(latent).sample\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image[0])\n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "def preprocess_images(images):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    images_ = []\n",
    "    for image in images:\n",
    "        image = preprocess(image)\n",
    "        images_.append(image)\n",
    "    images_ = torch.stack(images_)\n",
    "\n",
    "    return images_\n",
    "\n",
    "@torch.no_grad()\n",
    "def compare_images(images1, images2, prompts):\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    images1 = preprocess_images(images1)\n",
    "    images2 = preprocess_images(images2)\n",
    "\n",
    "    inputs = processor(text=prompts, images=images1, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    outputs = model(**inputs)\n",
    "    image_embds1 = outputs.image_embeds\n",
    "\n",
    "    inputs = processor(text=prompts, images=images2, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    outputs = model(**inputs)\n",
    "    image_embds2 = outputs.image_embeds\n",
    "\n",
    "    text_embds = outputs.text_embeds\n",
    "\n",
    "    clip_score1 = torch.nn.functional.cosine_similarity(image_embds1, text_embds).numpy().round(3)\n",
    "    clip_score2 = torch.nn.functional.cosine_similarity(image_embds2, text_embds).numpy().round(3)\n",
    "\n",
    "    loss_function = lpips.LPIPS(net='alex')\n",
    "    images1 = images1 * 2 - 1\n",
    "    images2 = images2 * 2 - 1\n",
    "\n",
    "    return pd.DataFrame({\"CLIP 1\": clip_score1, \"CLIP 2\": clip_score2,\n",
    "                         \"CLIP diff\": clip_score1 - clip_score2,\n",
    "                         \"LPIPS diff\": loss_function(images1, images2).squeeze().detach().numpy().round(3)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "autohalf_images = generate_autohalf_images(pipeline, prompts, seeds)\n",
    "\n",
    "delete_pipeline(pipeline)\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "half_images = generate_half_images(pipeline, prompts, seeds)\n",
    "\n",
    "delete_pipeline(pipeline)\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(device)\n",
    "\n",
    "images = generate_images(pipeline, prompts, seeds)\n",
    "\n",
    "delete_pipeline(pipeline)\n",
    "\n",
    "df1 = compare_images(autohalf_images, half_images, prompts)\n",
    "df2 = compare_images(autohalf_images, images, prompts)\n",
    "df3 = compare_images(half_images, images, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_prompts = artists[5:]\n",
    "prev_prompts = prompts[:5]\n",
    "new_prompts = [\"art\"] * 5\n",
    "\n",
    "@torch.no_grad()\n",
    "def erase_pipeline(pipeline, prev_prompts, new_prompts, retain_prompts):\n",
    "\n",
    "    device = pipeline.device\n",
    "\n",
    "    ca_layers = []\n",
    "    for name, module in pipeline.unet.named_modules():\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        ca_layers.append(module)\n",
    "\n",
    "    value_layers = [layer.to_v for layer in ca_layers]\n",
    "    target_layers = value_layers\n",
    "\n",
    "    key_layers = [layer.to_k for layer in ca_layers]\n",
    "    target_layers += key_layers\n",
    "    \n",
    "    prev_tokens = pipeline.tokenizer(prev_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prev_embds = pipeline.text_encoder(prev_tokens)[0].permute(0, 2, 1)\n",
    "    \n",
    "    new_tokens = pipeline.tokenizer(new_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    new_embds = pipeline.text_encoder(new_tokens)[0].permute(0, 2, 1)\n",
    "\n",
    "    lamb = 0.5\n",
    "    erase_scale = 1\n",
    "    preserve_scale = 0.1\n",
    "\n",
    "    m2 = (prev_embds @ prev_embds.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "    m3 = (new_embds @ prev_embds.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "    retain_tokens = pipeline.tokenizer(retain_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    retain_embds = pipeline.text_encoder(retain_tokens)[0].permute(0, 2, 1)\n",
    "\n",
    "    m2 += (retain_embds @ retain_embds.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "    m3 += (retain_embds @ retain_embds.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "\n",
    "    for target_layer in target_layers:\n",
    "        m1 = target_layer.weight @ m3\n",
    "        target_layer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(device)\n",
    "\n",
    "erased_pipeline = erase_pipeline(pipeline, prev_prompts, new_prompts, retain_prompts)\n",
    "\n",
    "erased_images = generate_images(erased_pipeline, prompts, seeds)\n",
    "\n",
    "erased_pipeline.text_encoder.half()\n",
    "erased_pipeline.vae.half()\n",
    "erased_pipeline.unet.half()\n",
    "\n",
    "erased_half_images = generate_half_images(erased_pipeline, prompts, seeds)\n",
    "erased_autohalf_images = generate_autohalf_images(erased_pipeline, prompts, seeds)\n",
    "\n",
    "df4 = compare_images(erased_autohalf_images, erased_half_images, prompts)\n",
    "df5 = compare_images(erased_autohalf_images, erased_images, prompts)\n",
    "df6 = compare_images(erased_half_images, erased_images, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIP 1</th>\n",
       "      <th>CLIP 2</th>\n",
       "      <th>CLIP diff</th>\n",
       "      <th>LPIPS diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.199</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.274</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.210</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.244</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.296</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLIP 1  CLIP 2  CLIP diff  LPIPS diff\n",
       "0   0.226   0.228     -0.002       0.002\n",
       "1   0.204   0.203      0.001       0.001\n",
       "2   0.211   0.211      0.000       0.001\n",
       "3   0.199   0.196      0.003       0.001\n",
       "4   0.147   0.148     -0.001       0.000\n",
       "5   0.274   0.274      0.000       0.000\n",
       "6   0.210   0.212     -0.002       0.002\n",
       "7   0.244   0.246     -0.002       0.006\n",
       "8   0.296   0.297     -0.001       0.000\n",
       "9   0.169   0.168      0.001       0.002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIP 1</th>\n",
       "      <th>CLIP 2</th>\n",
       "      <th>CLIP diff</th>\n",
       "      <th>LPIPS diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.199</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.274</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.210</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.244</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.296</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLIP 1  CLIP 2  CLIP diff  LPIPS diff\n",
       "0   0.226   0.227     -0.001       0.002\n",
       "1   0.204   0.202      0.002       0.002\n",
       "2   0.211   0.211      0.000       0.003\n",
       "3   0.199   0.196      0.003       0.004\n",
       "4   0.147   0.147      0.000       0.001\n",
       "5   0.274   0.273      0.001       0.001\n",
       "6   0.210   0.214     -0.004       0.005\n",
       "7   0.244   0.242      0.002       0.002\n",
       "8   0.296   0.297     -0.001       0.001\n",
       "9   0.169   0.168      0.001       0.007"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIP 1</th>\n",
       "      <th>CLIP 2</th>\n",
       "      <th>CLIP diff</th>\n",
       "      <th>LPIPS diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.228</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.148</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.274</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.297</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLIP 1  CLIP 2  CLIP diff  LPIPS diff\n",
       "0   0.228   0.227      0.001       0.001\n",
       "1   0.203   0.202      0.001       0.004\n",
       "2   0.211   0.211      0.000       0.003\n",
       "3   0.196   0.196      0.000       0.004\n",
       "4   0.148   0.147      0.001       0.001\n",
       "5   0.274   0.273      0.001       0.001\n",
       "6   0.212   0.214     -0.002       0.007\n",
       "7   0.246   0.242      0.004       0.008\n",
       "8   0.297   0.297      0.000       0.001\n",
       "9   0.168   0.168      0.000       0.008"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/artists1734_prompts.csv\")\n",
    "\n",
    "artists = list(df.artist.unique())\n",
    "random.shuffle(artists)\n",
    "\n",
    "prompts = artists[:20]\n",
    "seeds = [random.randint(0, 5000) for _ in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fcde0263534e49b2e780fce4a82613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de5b8709aaf471cba0b0dac3ed71b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_images(pipeline, prompts, seeds):\n",
    "\n",
    "    device = pipeline.unet.device\n",
    "\n",
    "    images = []\n",
    "    for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "        scheduler.set_timesteps(100)\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "\n",
    "        token = pipeline.tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "        embd = pipeline.text_encoder(token)[0].to(device)\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        latent = torch.randn((1, 4, 64, 64), generator=generator).to(device)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "\n",
    "        for t in scheduler.timesteps:\n",
    "\n",
    "            latent_input = torch.cat([latent] * 2)\n",
    "            latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "            noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "            cond_noise, uncond_noise = noise.chunk(2)\n",
    "            noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "        latent /= 0.18215\n",
    "        image = pipeline.vae.decode(latent).sample\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image[0])\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline.unet.to(device)\n",
    "pipeline.vae.to(device)\n",
    "\n",
    "images = generate_images(pipeline, prompts, seeds)\n",
    "\n",
    "def delete_pipeline(pipeline):\n",
    "    del pipeline.vae\n",
    "    del pipeline.tokenizer\n",
    "    del pipeline.text_encoder\n",
    "    del pipeline.unet\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "delete_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e77639b29cc4b26a64451152fd7e73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a0c7f04efc4eb68b22b7147e95aa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_images2(pipeline, prompts, seeds):\n",
    "\n",
    "    device = pipeline.unet.device\n",
    "\n",
    "    images = []\n",
    "    for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "        scheduler.set_timesteps(100)\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "\n",
    "        token = pipeline.tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "        embd = pipeline.text_encoder(token)[0].to(device).half()\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        latent = torch.randn((1, 4, 64, 64), generator=generator).to(device).half()\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "\n",
    "        for t in scheduler.timesteps:\n",
    "\n",
    "            latent_input = torch.cat([latent] * 2)\n",
    "            latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "\n",
    "            noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "            cond_noise, uncond_noise = noise.chunk(2)\n",
    "            noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "\n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "\n",
    "        latent /= 0.18215\n",
    "        image = pipeline.vae.decode(latent).sample\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image[0])\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline.unet.to(device).half()\n",
    "pipeline.vae.to(device).half()\n",
    "\n",
    "images2 = generate_images2(pipeline, prompts, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57249c501dc847ae88901128caf40255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acb785eb26948c29409618975b10f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retain_prompts = artists[10:100]\n",
    "prev_prompts = prompts[:10]\n",
    "new_prompts = [\"art\"] * 10\n",
    "\n",
    "@torch.no_grad()\n",
    "def erase_pipeline(pipeline, prev_prompts, new_prompts, retain_prompts):\n",
    "\n",
    "    device = pipeline.unet.device\n",
    "\n",
    "    ca_layers = []\n",
    "    for name, module in pipeline.unet.named_modules():\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        ca_layers.append(module)\n",
    "\n",
    "    value_layers = [layer.to_v for layer in ca_layers]\n",
    "    target_layers = value_layers\n",
    "\n",
    "    key_layers = [layer.to_k for layer in ca_layers]\n",
    "    target_layers += key_layers\n",
    "    \n",
    "    pre_tokens = pipeline.tokenizer(prev_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    prev_embds = pipeline.text_encoder(pre_tokens)[0].permute(0, 2, 1).to(device)\n",
    "    \n",
    "    new_tokens = pipeline.tokenizer(new_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    new_embds = pipeline.text_encoder(new_tokens)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "    lamb = 0.5\n",
    "    erase_scale = 1\n",
    "    preserve_scale = 0.1\n",
    "\n",
    "    m2 = (prev_embds @ prev_embds.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "    m3 = (new_embds @ prev_embds.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "    retain_tokens = pipeline.tokenizer(retain_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    retain_embds = pipeline.text_encoder(retain_tokens)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "    m2 += (retain_embds @ retain_embds.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "    m3 += (retain_embds @ retain_embds.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "\n",
    "    for target_layer in target_layers:\n",
    "        m1 = target_layer.weight @ m3\n",
    "        target_layer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).detach())\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "delete_pipeline(pipeline)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline.unet.to(device)\n",
    "pipeline.vae.to(device)\n",
    "\n",
    "pipeline = erase_pipeline(pipeline, prev_prompts, new_prompts, retain_prompts)\n",
    "\n",
    "images3 = generate_images(pipeline, prompts, seeds)\n",
    "\n",
    "delete_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edcaffe7a4c45838743873023cb27b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d22c0044b04c51aaa6b4ee10050019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def erase_pipeline2(pipeline, prev_prompts, new_prompts, retain_prompts):\n",
    "\n",
    "    device = pipeline.unet.device\n",
    "\n",
    "    ca_layers = []\n",
    "    for name, module in pipeline.unet.named_modules():\n",
    "        if name[-5:] != \"attn2\": continue\n",
    "        ca_layers.append(module)\n",
    "\n",
    "    value_layers = [layer.to_v for layer in ca_layers]\n",
    "    target_layers = value_layers\n",
    "\n",
    "    key_layers = [layer.to_k for layer in ca_layers]\n",
    "    target_layers += key_layers\n",
    "    \n",
    "    pre_tokens = pipeline.tokenizer(prev_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    prev_embds = pipeline.text_encoder(pre_tokens)[0].permute(0, 2, 1).to(device)\n",
    "    \n",
    "    new_tokens = pipeline.tokenizer(new_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    new_embds = pipeline.text_encoder(new_tokens)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "    lamb = 0.5\n",
    "    erase_scale = 1\n",
    "    preserve_scale = 0.1\n",
    "\n",
    "    m2 = (prev_embds @ prev_embds.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m2 += lamb * torch.eye(m2.shape[0], device=device)\n",
    "\n",
    "    m3 = (new_embds @ prev_embds.permute(0, 2, 1)).sum(0) * erase_scale\n",
    "    m3 += lamb * torch.eye(m3.shape[0], device=device)\n",
    "\n",
    "    retain_tokens = pipeline.tokenizer(retain_prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    retain_embds = pipeline.text_encoder(retain_tokens)[0].permute(0, 2, 1).to(device)\n",
    "\n",
    "    m2 += (retain_embds @ retain_embds.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "    m3 += (retain_embds @ retain_embds.permute(0, 2, 1)).sum(0) * preserve_scale\n",
    "\n",
    "    for target_layer in target_layers:\n",
    "        m1 = target_layer.weight.to(torch.float32) @ m3\n",
    "        target_layer.weight = torch.nn.Parameter((m1 @ torch.inverse(m2)).to(torch.float16).detach())\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline.unet.to(device).half()\n",
    "pipeline.vae.to(device).half()\n",
    "\n",
    "pipeline = erase_pipeline2(pipeline, prev_prompts, new_prompts, retain_prompts)\n",
    "\n",
    "images4 = generate_images2(pipeline, prompts, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a47e4ce75940a6adc2f1bb1100941f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15d0de207ba4b88ae3d8152acc966ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
    "pipeline = erase_pipeline(pipeline, prev_prompts, new_prompts, retain_prompts)\n",
    "\n",
    "pipeline.unet.to(device).half()\n",
    "pipeline.vae.to(device).half()\n",
    "\n",
    "images5 = generate_images2(pipeline, prompts, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images, score=None):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    images_ = []\n",
    "    for image in images:\n",
    "        image = preprocess(image)\n",
    "        images_.append(image)\n",
    "    images_ = torch.stack(images_)\n",
    "    if score == \"lpips\": images_ = images_ * 2 - 1\n",
    "    elif score == \"clip\": pass\n",
    "\n",
    "    return images_\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_LPIPS(images1, images2):\n",
    "\n",
    "    images1 = preprocess_images(images1, \"lpips\")\n",
    "    images2 = preprocess_images(images2, \"lpips\")\n",
    "    \n",
    "    loss_function = lpips.LPIPS(net='alex')\n",
    "    return loss_function(images1, images2).squeeze().detach().numpy().round(3)\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_CLIP(images, prompts):\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    images = preprocess_images(images, \"clip\")\n",
    "    inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    image_embds = outputs.image_embeds\n",
    "    text_embds = outputs.text_embeds\n",
    "    \n",
    "    return torch.nn.functional.cosine_similarity(image_embds, text_embds).numpy().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LPIPS 1-2</th>\n",
       "      <th>CLIP 1-2</th>\n",
       "      <th>LPIPS 3-4</th>\n",
       "      <th>CLIP 3-4</th>\n",
       "      <th>LPIPS 3-5</th>\n",
       "      <th>CLIP 3-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LPIPS 1-2  CLIP 1-2  LPIPS 3-4  CLIP 3-4  LPIPS 3-5  CLIP 3-5\n",
       "0       0.000     0.000      0.022     0.005      0.031     0.002\n",
       "1       0.000    -0.002      0.008     0.001      0.152    -0.007\n",
       "2       0.026    -0.011      0.010    -0.002      0.087    -0.010\n",
       "3       0.008    -0.003      0.066    -0.006      0.140     0.028\n",
       "4       0.000    -0.001      0.027    -0.003      0.097    -0.007\n",
       "5       0.000     0.000      0.066    -0.002      0.031     0.009\n",
       "6       0.000     0.001      0.020    -0.003      0.035    -0.008\n",
       "7       0.003     0.002      0.024     0.006      0.013    -0.004\n",
       "8       0.003    -0.001      0.009     0.003      0.015     0.000\n",
       "9       0.005    -0.003      0.196     0.020      0.179     0.003\n",
       "10      0.000    -0.001      0.019     0.004      0.033     0.003\n",
       "11      0.001     0.000      0.012     0.001      0.015    -0.003\n",
       "12      0.009    -0.003      0.064     0.008      0.133     0.000\n",
       "13      0.003     0.004      0.391     0.011      0.175    -0.030\n",
       "14      0.039    -0.004      0.084     0.004      0.095    -0.015\n",
       "15      0.001    -0.003      0.067     0.014      0.022     0.011\n",
       "16      0.000     0.000      0.005    -0.002      0.007     0.001\n",
       "17      0.002     0.002      0.308     0.001      0.359    -0.015\n",
       "18      0.058     0.003      0.227    -0.042      0.238     0.012\n",
       "19      0.002     0.003      0.037     0.011      0.076    -0.007"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"LPIPS 1-2\": measure_LPIPS(images, images2),\n",
    "              \"CLIP 1-2\": measure_CLIP(images, prompts) - measure_CLIP(images2, prompts),\n",
    "              \"LPIPS 3-4\": measure_LPIPS(images3, images4),\n",
    "              \"CLIP 3-4\": measure_CLIP(images3, prompts) - measure_CLIP(images4, prompts),\n",
    "              \"LPIPS 3-5\": measure_LPIPS(images3, images5),\n",
    "              \"CLIP 3-5\": measure_CLIP(images3, prompts) - measure_CLIP(images5, prompts),\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

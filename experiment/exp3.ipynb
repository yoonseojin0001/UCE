{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lpips\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/artists1734_prompts.csv\")\n",
    "\n",
    "artists = list(df.artist.unique())\n",
    "random.shuffle(artists)\n",
    "\n",
    "prompts = artists[:20]\n",
    "seeds = [random.randint(0, 5000) for _ in prompts]\n",
    "\n",
    "prev_prompts = prompts[:10]\n",
    "new_prompts = [\"art\"] * 10\n",
    "retain_prompts = artists[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline):\n",
    "    del pipeline.vae\n",
    "    del pipeline.tokenizer\n",
    "    del pipeline.text_encoder\n",
    "    del pipeline.unet\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def preprocess_images(images):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    images_ = []\n",
    "    for image in images:\n",
    "        image = preprocess(image)\n",
    "        images_.append(image)\n",
    "    images_ = torch.stack(images_)\n",
    "\n",
    "    return images_\n",
    "\n",
    "@torch.no_grad()\n",
    "def compare_images(images1, images2, prompts):\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    images1 = preprocess_images(images1)\n",
    "    images2 = preprocess_images(images2)\n",
    "\n",
    "    inputs = processor(text=prompts, images=images1, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    outputs = model(**inputs)\n",
    "    image_embds1 = outputs.image_embeds\n",
    "\n",
    "    inputs = processor(text=prompts, images=images2, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    outputs = model(**inputs)\n",
    "    image_embds2 = outputs.image_embeds\n",
    "\n",
    "    text_embds = outputs.text_embeds\n",
    "\n",
    "    clip_score1 = torch.nn.functional.cosine_similarity(image_embds1, text_embds).numpy().round(3)\n",
    "    clip_score2 = torch.nn.functional.cosine_similarity(image_embds2, text_embds).numpy().round(3)\n",
    "\n",
    "    loss_function = lpips.LPIPS(net='alex')\n",
    "    images1 = images1 * 2 - 1\n",
    "    images2 = images2 * 2 - 1\n",
    "\n",
    "    return pd.DataFrame({\"CLIP 1\": clip_score1, \"CLIP 2\": clip_score2,\n",
    "                         \"CLIP diff\": clip_score1 - clip_score2,\n",
    "                         \"LPIPS diff\": loss_function(images1, images2).squeeze().detach().numpy().round(3)})\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_images(pipeline, prompts, seeds, step_count=100):\n",
    "\n",
    "    device = pipeline.device\n",
    "    images = []\n",
    "    for idx, prompt in enumerate(tqdm(prompts)):\n",
    "\n",
    "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "        scheduler.set_timesteps(step_count)\n",
    "\n",
    "        prompt = [prompt] + [\"\"]\n",
    "        \n",
    "        token = pipeline.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "        \n",
    "        embd = pipeline.text_encoder(token)[0]\n",
    "        \n",
    "        generator = torch.Generator(device=device).manual_seed(seeds[idx])\n",
    "        \n",
    "        latent = torch.randn((1, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "        latent *= scheduler.init_noise_sigma\n",
    "        \n",
    "        for t in scheduler.timesteps:\n",
    "            latent_input = torch.cat([latent] * 2)\n",
    "            latent_input = scheduler.scale_model_input(latent_input, timestep=t)\n",
    "            \n",
    "            noise = pipeline.unet(latent_input, t, encoder_hidden_states=embd).sample\n",
    "            cond_noise, uncond_noise = noise.chunk(2)\n",
    "            noise = uncond_noise + 7.5 * (cond_noise - uncond_noise)\n",
    "            \n",
    "            latent = scheduler.step(noise, t, latent).prev_sample\n",
    "        \n",
    "        image = pipeline.vae.decode(latent).sample\n",
    "        image = ((image + 1) / 2).clamp(0, 1).permute(0, 2, 3, 1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(image[0])\n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f628e63f44443bba60306769f739358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8020b1356b0c4515b33cedf31f7903bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51093a6099f7496c90e9042b8a3fa81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "step100_images = generate_images(pipeline, prompts, seeds)\n",
    "step50_images = generate_images(pipeline, prompts, seeds, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\yoonj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIP 1</th>\n",
       "      <th>CLIP 2</th>\n",
       "      <th>CLIP diff</th>\n",
       "      <th>LPIPS diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.317</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.228</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.288</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.260</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.278</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.254</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.268</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.291</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.262</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.172</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.240</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.246</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CLIP 1  CLIP 2  CLIP diff  LPIPS diff\n",
       "0    0.266   0.267     -0.001       0.003\n",
       "1    0.317   0.319     -0.002       0.005\n",
       "2    0.228   0.230     -0.002       0.001\n",
       "3    0.125   0.127     -0.002       0.002\n",
       "4    0.288   0.289     -0.001       0.002\n",
       "5    0.190   0.211     -0.021       0.063\n",
       "6    0.260   0.267     -0.007       0.050\n",
       "7    0.246   0.244      0.002       0.012\n",
       "8    0.278   0.282     -0.004       0.002\n",
       "9    0.212   0.209      0.003       0.002\n",
       "10   0.254   0.254      0.000       0.005\n",
       "11   0.285   0.285      0.000       0.002\n",
       "12   0.329   0.330     -0.001       0.025\n",
       "13   0.268   0.268      0.000       0.001\n",
       "14   0.291   0.287      0.004       0.001\n",
       "15   0.262   0.280     -0.018       0.101\n",
       "16   0.172   0.163      0.009       0.037\n",
       "17   0.240   0.239      0.001       0.000\n",
       "18   0.252   0.255     -0.003       0.010\n",
       "19   0.246   0.243      0.003       0.012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_images(step100_images, step50_images, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
